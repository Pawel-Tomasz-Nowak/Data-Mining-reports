---
title: "Sprawozdanie z listy 2"
subtitle: "Eksploracja danych"
author: "Marta Stankiewicz (282244)  \n Paweł Nowak (282223)"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage[OT4]{polski}
   - \usepackage[utf8]{inputenc}
   - \usepackage{graphicx}
   - \usepackage{float}
output: 
  pdf_document:
    toc: true
    fig_caption: true
    fig_width: 5 
    fig_height: 4 
    number_sections: true
fontsize: 12pt 
lof: true
lot: true
---


---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(fig.pos = "H", out.extra ='', fig.align = "center")
knitr::opts_chunk$set(dev.args = list(encoding = "CP1250.ENC"))


```


```{r importing libraries, warning=FALSE, include=FALSE}
# Importing necessary libraries.
library(datasets)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(arules)
library(knitr)
library(tidyr)
library(corrplot)
```



```{r data reading, include=FALSE}
# Reading the 'iris' data set.
data("iris") 
iris_df <- iris


num.cols <- colnames(iris_df %>% dplyr::select( where(is.numeric))) # Find the numerical variables.
print(num.cols)
```


# Ocena zdolności separacyjnych zmiennych, dyskretyzacja zmiennych ciągłych
## Ocena zdolności dyskryminacyjnych zmiennych ciągłych. {#discriminative_power_compare}

W celu zbadania zdolności dyskryminacyjnej cech, posłużymy się wykresem skrzypcowo-pudełkowym (tj. wykresem skrzypcowym wraz z wykresem pudełkowym). 
```{r violin boxplots, echo=FALSE, fig.width = 9, fig.height = 8, fig.cap = "Wykresy skrzypcowo-pudełkowe dla zmiennych ciągłych"}
  # Create a violin-with-box plot for each continous variable and arrange them in a figure.
    violin.plot.box_maker <- function(var){
    ggplot(data = iris_df, 
           aes(x = Species, y = iris_df[[var]], fill = Species)) +
      geom_violin(alpha = 0.2) +
      geom_boxplot(width = 0.15) +
        ylab(gsub("\\.", " ", var)) + 
        labs(title = paste("Wykres skrzypcowo-pudełkowy zmiennej\n",var))
    
    }
    
    grid.arrange(violin.plot.box_maker("Sepal.Length"), violin.plot.box_maker("Sepal.Width"),
               violin.plot.box_maker("Petal.Length"),
              violin.plot.box_maker("Petal.Width")
              )

```

Z wykresów \ref{fig:violin boxplots} wnioskujemy, że największe zdolności dyskryminacyjne wykazuje zmienna *Petal.Width*. Z kolei najmniejsze zdolności do separacji gatunków obserwujemy u zmiennej *Sepal.Width*.
\newpage

## Porównanie róznych metod dyskretyzacji nienadzorowanej. {#discretization_intro}
Dla wymienionych wyżej zmiennych (tj. *Petal.Width* oraz *Sepal.Width*) zastosujemy teraz różne techniki przedziałowania (dyskretyzacji) według, odpowiednio, **stałej szerokości** przedziału, **równej częstości**, **algorytmu K-średnich**, **stałych granicach** przedziałów ustalonych przez użytkownika.


```{r discretization, include = FALSE}
var_discretization <- function(df, var){
  # Estimate the mean and sd of the variable (we'll use these values for fixed-type discretization)
  var.vec <- df[[var]]
  
  mean.est <- mean(var.vec)
  sd.est <- sd(var.vec)
  
  
  # Oparta na równej częstotliwości.
  var.discr.freq <- discretize(var.vec, 
                               method = "frequency", 
                               breaks = 3,
                               label = c("Krótka", "Średnia", "Długa"))
  
  # equal-width discretization
  var.discr.width <- discretize(var.vec, 
                                method = "interval", 
                                breaks = 3,
                                label = c("Krótka", "Średnia", "Długa"))
  
  # cluster discretization
  var.discr.cluster <- discretize(var.vec, 
                                  method = "cluster", 
                                  breaks = 3,
                                  label = c("Krótka", "Średnia", "Długa"))
  
  # user-specified breaks bounds.
  var.discr.user <- discretize(var.vec,
                               method = "fixed",
                               breaks = c(-Inf, mean.est-sd.est, mean.est+sd.est, Inf),
                               label = c("Krótka", "Średnia", "Długa"))

  
  
  # Store discretization result in a dataframe.
  df.discr_vals <- data.frame("equal_frequency" = var.discr.freq,
                              "equal_width" = var.discr.width,
                              "cluster" = var.discr.cluster,
                              "fixed_bounds"= var.discr.user
                              )
  
  # Rename the columns.
  renamed.columns <- sapply(colnames(df.discr_vals), function(x){ paste(var, " ", x) })
  colnames(df.discr_vals) = renamed.columns
  
  return(df.discr_vals)
  }

```

```{r discretization method comparison, echo=FALSE}
most_common_class <- function(row) {
  # Zlicz ile razy każda klasa występuje w wierszu
  tab <- table(as.character(row))
  # Zwróć klasę z największą liczbą wystąpień (w przypadku remisu bierze pierwszą)
  names(which.max(tab))
}

do_class_match <- function(row){
  # Porównaj każdy element z ostatnim elementem w wierszu
  return(as.integer(row[-length(row)] == row[length(row)]))
}

evaluate_method <- function(df, var){
  # Discretize the variable.
  df.discr_vals <- var_discretization(df, var)
  
  # Find the most common class row-wise.
  df.discr_vals$most.common.class <- apply(df.discr_vals, 1, most_common_class)
    
  # For each method, find the percentage of matches.
  result_match <- t(apply(df.discr_vals, 1, do_class_match)) %>% 
    apply(2, function(x){ 100 * sum(x) / nrow(df.discr_vals)}) %>%
    round(2)
  
  
  # The table with the metrics of accuracy of discretization.
  acc_table <- kable(t(data.frame(Skutecznosc = result_match)), 
                col.names = c("Przedziałowanie według równej częstotliwość", 
                              "Przedziałowanie według równej szerokości", 
                              "Dyskretyzacja oparta na algorytmie K-średnich", 
                              "Stałe granice przedziału"), 
                row.names = FALSE, 
                caption = paste("Skuteczność wybranych metod dyskretyzacji dla zmiennej", gsub("\\.", " ",var)))
  
  # Return the table
  return(acc_table)
}


```

### Metodologia oceny skuteczności dyskretyzacji
Aby ocenić skuteczność każdej ze [wspomnianych metod](#discretization_intro), przyjęliśmy następującą metodologię.
Najpierw dokonaliśmy przedziałowania każdej obserwacji, korzystając ze wszystkich metod, a następnie wybraliśmy tę klasę, która występuje najczęściej (w przypadku tzw. "remisu" wybierana jest dowolna klasa). Następnie sprawdzaliśmy, w ilu przypadkach wynik przedziałowania każdej metody zgadzał się ze zagregowaną klasą. Tę liczbę podzieliliśmy przez liczbę wszystkich przypadków, aby uzyskać procent zgodności danej metody dyskretyzacji. Porównanie róznych metod przedziałowania zostały przedstawione poniżej
```{r binning_results1, echo=FALSE}
discretization_results <- evaluate_method(iris, "Sepal.Width")
discretization_results
```


```{r binning_results2, echo=FALSE}
discretization_results <- evaluate_method(iris, "Petal.Width")
discretization_results
```

### Wnioski dotyczące skuteczności przedziałowania
<!-- Czemu latex nie widzi referencji do binning_results1? -->
Z tabel \ref{tab:binning_results1} oraz \ref{tab:binning_results2} możemy wywnioskować, że w obu przypadkach największą skutecznością charakteryzuje się metoda dyskretyzacji oparta na **algorytmie K-średnich**. Z kolei najgorszą skuteczność przedziałowania obserwujemy dla metody opartej na **stałych granicach** przedziału.
Wyniki dyskretyzacji zastosowanej dla zmiennej *Petal.Width* znaczącą rożnią się od wyników przedziałowania zastosowanego dla atrybutu *Sepal.Width*. 
Jest to zgodne z intuicją — jak wykazaliśmy [wcześniej](#discriminative_power_compare), najgorsze zdolności separacyjne klas wykazuje właśnie zmienna **Sepal.Width**, co znacząco wpływa na niską skuteczność metod przedziałowania. Analogiczna zależność występuje w przypadku cechy **Petal.Width**, która z kolei charakteryzowała się wysokimi zdolnościami dyskryminacyjnymi, co przełożyło się na wysoką dokładność podejść dyskretyzacji. 

# Analiza składowych głównych
```{r new_dataset_loading, include=FALSE}
# Loading the new dataset.
CQL_df <- read.csv(file = "uaScoresDataFrame.csv") %>%
            mutate(X = NULL)

# Wybierz same cechy ilościowe.
num.cols_CQL <- colnames(select(CQL_df, where(is.numeric)))

```

## Porównanie wariancji zmiennych ilościowych.
W celu porównania wariancji wszystkich zmiennych ilościowych ze zbioru *uaScoresDataFrame*, posłużymy się wykresami pudełkowymi.
```{r variance_comparison_before, echo=FALSE, fig.cap="Wykresy pudelkowe zmiennych ciągłych przed zastosowaniem standaryzacją",fig.width = 9, fig.height  = 8}
CQL_df_longer <- CQL_df %>% pivot_longer( num.cols_CQL, values_to = "Realizacja", names_to = "Zmienna")


ggplot(CQL_df_longer, aes(y = Realizacja, fill = Zmienna)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykres pudełkowy zmiennych ilościowych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL))

```
Przyjrzyjmy się wykresowi \ref{fig:variance_comparison_before}. Obserwujemy wysokie zróżnicowanie wariancji. Z jednej strony w badanym zbiorze występują cechy o niskiej dewiacji, która charakteryzuje chociażby zmienną *Commute*. Z drugiej obecność takich zmiennych jak *Environmental.Quality* i *Venture.Capital* pokazują, że nie brakuję atrybutów o wysokiej wariancji. W celu ujednolicenia wariancji zmiennych, konieczne będzie zastosowanie standaryzacji.

```{r, feature standarization, echo = FALSE}

df_scaled <- apply(CQL_df[, num.cols_CQL], 
                   MARGIN = 2, 
                   FUN = function(x) {(x - mean(x))/sd(x)}
)

CQL_df_scaled <- as.data.frame(CQL_df)
CQL_df_scaled[, num.cols_CQL] <- df_scaled

```


Na poniższym wykresie \ref{fig:variance_comparison_after} pudełkowym widoczne są efekty standaryzacji zastosowane dla zmiennych ze zbioru danych.

```{r variance_comparison_after, echo=FALSE, fig.cap="Wykresy pudelkowe zmiennych ciągłych po zastosowaniu standaryzacji", fig.width = 9, fig.height  = 8}
CQL.df_scaled.longer <- CQL_df_scaled %>% pivot_longer( num.cols_CQL, values_to = "Realizacja", names_to = "Zmienna")


ggplot(CQL.df_scaled.longer, aes(y = Realizacja, fill = Zmienna)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykres pudełkowy zmiennych ilościowych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL))

```
## Badanie korelacji między zmiennymi.
Po dokonaniu standaryzacji zmiennych ilościowych, zbadamy jeszcze, jak silne są korelacje między atrybutami w zbiorze danych. Występowanie silnej korelacji świadczy o występowaniu reduntantnych zmiennych. Taka redundancja może zostać wyelimonowana za pomocą analizy składowych głównych. Aby poprawić czytelność wykresu, nazwy zmiennych zostały pominięte, a wartość współczynnika została przeskalowana do przedzialu [0;1].

<!-- Rozmiary wykresu korelacji do poprawy, bo tytułu nie widać  -->
```{r corr matrix, echo = FALSE, fig.cap = "Macierz korelacji dla zmiennych ciągłych"}

corr_matrix <- abs(cor(x = CQL_df[num.cols_CQL], use = "complete.obs"))

corrplot(corr_matrix, method = "square", type = "full", tl.cex = 0.8,
         title = "Macierz korelacji dla zmiennych ciągłych", tl.pos = FALSE,
         col.lim = c(0,1))
```

Na podstawie rysunku \ref{fig:corr matrix} można zauważyć, że w zdecydowanej większości przypadków korelacje pomiędzy zmiennymi są stosunkowo słabe. Niemniej jednak, występują również przypadki skrajne, w których wartości współczynnika korelacji — rozpatrywane w sensie bezwzględnym — zbliżają się do jedności, wskazując na silne liniowe powiązania między wybranymi zmiennymi. W związku z tym należy oczekiwać, że redukcja wymiarowości będzie wymagała uwzględnienia relatywnie dużej liczby składowych głównych, aby osiągnąć zakładaną frakcję wyjaśnianej wariancji.

## Wyznaczanie składowych głównych.
Dla rozważanego zbioru zmiennych ciągłych wyznaczone zostaną składowe główne. Zostaną porównane ich rozrzut, stopień wyjaśnianej wariancji. Na sam koniec zostanie dobrana odpowiednia liczba składowych głównych, aby całkowita wyjaśniania przez nie wariancja wyniosła co najmniej 80% lub 90% odpowiednio.
```{r principal components computation, echo = FALSE}
prin.comp <- prcomp(x = CQL_df[num.cols_CQL], retx = TRUE) # Wyznacz składowe główne.

PCA_loadings <- prin.comp$rotation # Wyznacz ładunki składowych głównych.
PCA_variances <- (prin.comp$sdev)^2 # Znajdź wariancje składowych głównych.
PCA_rotated.df <- as.data.frame(prin.comp$x) # Wyznacz wartości składowych głównych.

# Przekształć rotated.df to postaci długiej (long form)
rotated.df_long <- pivot_longer(PCA_rotated.df,cols = everything(), names_to = "Składowa", values_to = "Wartość") 

# Liczba składowych.
n.PCA <- ncol(PCA_rotated.df)
levels.name <- paste(c("PC"), (1:n.PCA), sep = "")

levels.name <- factor(x = rotated.df_long$Składowa,
                                   levels = levels.name, 
                                   ordered = TRUE)

rotated.df_long$Składowa <- levels.name

```

```{r PC variance visualization, echo = FALSE, fig.height =8, fig.width = 9, fig.cap = "Wykresy pudełkowe dla składowych głównych"}
ggplot(rotated.df_long, aes(y = Wartość, fill = Składowa)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykres pudełkowy składowych głównych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL))

```








```{r variances_explained_plot, echo = FALSE, fig.height=8, fig.width = 9, fig.cap = "Wariancje wyjaśniane przez poszczególne zmienne"}
# Wariancja wyjaśniana przez poszczegolne składowe.
var_explained.df<- data.frame(Wyjaśniana_wariancja = PCA_variances/sum(PCA_variances),
                              Numer_składowej = levels.name)


p1 <- ggplot(var_explained.df, mapping = aes(x = Numer_składowej, y = Wyjaśniana_wariancja)) + 
  geom_bar(stat = "identity") +ggtitle("Wariancje wyjaśniane przez składowe główne")
print(p1)

```

```{r cum.variances_explained_plot, echo = FALSE, fig.height=8, fig.width = 9, fig.cap = "Łączna wyjaśniana wariancja"}
# Łączna wyjaśniana wariancja przez składowe główne.
cum.var_explained.df<- data.frame(Łączna_wyjaśniana_wariancja = cumsum(PCA_variances)/sum(PCA_variances),
                              Numer_składowej = levels.name)

p2 <- ggplot(cum.var_explained.df, mapping = aes(x = Numer_składowej, y = Łączna_wyjaśniana_wariancja)) + 
  geom_bar(stat = "identity") + ggtitle("Łączna wyjaśniana wariancja")

print(p2)
```



