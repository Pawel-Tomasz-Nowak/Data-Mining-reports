---
title: "Sprawozdanie z listy 2"
subtitle: "Eksploracja danych"
author: "Marta Stankiewicz (282244)  \n Paweł Nowak (282223)"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage[OT4]{polski}
   - \usepackage[utf8]{inputenc}
   - \usepackage{graphicx}
   - \usepackage{float}
output: 
  pdf_document:
    toc: true
    fig_caption: true
    fig_width: 5 
    fig_height: 4 
    number_sections: true
fontsize: 12pt 
lof: true
lot: true
---

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(fig.pos = "H", out.extra = '', fig.align = "center")
knitr::opts_chunk$set(dev.args = list(encoding = "CP1250.ENC"))
```


```{r importing libraries, warning=FALSE, include=FALSE}
# Importowanie niezbędnych bibliotek
library(datasets)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(arules)
library(knitr)
library(tidyr)
library(corrplot)
library(ggrepel)
```



```{r data reading, include=FALSE}
# Reading the 'iris' data set.
data("iris") 
iris_df <- iris


num.cols <- colnames(iris_df %>% dplyr::select( where(is.numeric))) # Find the numerical variables.
print(num.cols)
```


# Ocena zdolności separacyjnych zmiennych, dyskretyzacja zmiennych ciągłych
## Ocena zdolności dyskryminacyjnych zmiennych ciągłych. {#discriminative_power_compare}

W celu zbadania zdolności dyskryminacyjnej cech, posłużymy się wykresem skrzypcowo-pudełkowym (tj. wykresem skrzypcowym wraz z wykresem pudełkowym). 
```{r violin boxplots, echo=FALSE, fig.width = 9, fig.height = 8, fig.cap = "Wykresy skrzypcowo-pudełkowe dla zmiennych ciągłych"}
  # Create a violin-with-box plot for each continous variable and arrange them in a figure.
    violin.plot.box_maker <- function(var){
    ggplot(data = iris_df, 
           aes(x = Species, y = iris_df[[var]], fill = Species)) +
      geom_violin(alpha = 0.2) +
      geom_boxplot(width = 0.15) +
        ylab(gsub("\\.", " ", var)) + 
        labs(title = paste("Wykres skrzypcowo-pudełkowy zmiennej\n",var))
    }
    
    grid.arrange(violin.plot.box_maker("Sepal.Length"), violin.plot.box_maker("Sepal.Width"),
               violin.plot.box_maker("Petal.Length"),
              violin.plot.box_maker("Petal.Width")
              )

```

Z wykresów \ref{fig:violin boxplots} wnioskujemy, że największe zdolności dyskryminacyjne wykazuje zmienna *Petal.Width*. Z kolei najmniejsze zdolności do separacji gatunków obserwujemy u zmiennej *Sepal.Width*.

## Porównanie róznych metod dyskretyzacji nienadzorowanej. {#discretization_intro}
Dla wymienionych wyżej zmiennych (tj. *Petal.Width* oraz *Sepal.Width*) zastosujemy teraz różne techniki przedziałowania (dyskretyzacji) według, odpowiednio, **stałej szerokości** przedziału, **równej częstości**, **algorytmu K-średnich**, **stałych granicach** przedziałów ustalonych przez użytkownika.


```{r discretization, include = FALSE}
var_discretization <- function(df, var) {
  var.vec <- df[[var]]
  
  # Oszacuj średnią oraz wariancję (użyjemy ich do okreslenia własnych granic przedziałów)
  mean.est <- mean(var.vec)
  sd.est <- sd(var.vec)
  
  
  # Przedziałowanie oparte na równej częstotliwości przedziałów.
  var.discr.freq <- discretize(var.vec, 
                               method = "frequency", 
                               breaks = 3,
                               label = c("Krótka", "Średnia", "Długa"))
  
  # Przedziałowanie oparte na przedziałach równej długości.
  var.discr.width <- discretize(var.vec, 
                                method = "interval", 
                                breaks = 3,
                                label = c("Krótka", "Średnia", "Długa"))
  
  # Przedziałowanie oparte na klasteryzacji.
  var.discr.cluster <- discretize(var.vec, 
                                  method = "cluster", 
                                  breaks = 3,
                                  label = c("Krótka", "Średnia", "Długa"))
  
  # Przedziałowanie na podstawie wartości granic przedziałów użytkownika.
  var.discr.user <- discretize(var.vec,
                               method = "fixed",
                               breaks = c(-Inf, mean.est-sd.est, mean.est+sd.est, Inf),
                               label = c("Krótka", "Średnia", "Długa"))

  
  
  # Wyniki przedziałowania umieść w ramce danych.
  df.discr_vals <- data.frame("equal_frequency" = var.discr.freq,
                              "equal_width" = var.discr.width,
                              "cluster" = var.discr.cluster,
                              "fixed_bounds"= var.discr.user
                              )
  
  # Rename the columns.
  renamed.columns <- sapply(colnames(df.discr_vals), function(x){ paste(var, " ", x) })
  colnames(df.discr_vals) = renamed.columns
  
  return(df.discr_vals)
  }

```

```{r discretization method comparison, echo=FALSE}
most_common_class <- function(row) {
  # Zlicz ile razy każda klasa występuje w wierszu
  tab <- table(as.character(row))
  # Zwróć klasę z największą liczbą wystąpień (w przypadku remisu bierze pierwszą)
  names(which.max(tab))
}

do_class_match <- function(row){
  # Porównaj każdy element z ostatnim elementem w wierszu
  return(as.integer(row[-length(row)] == row[length(row)]))
}

evaluate_method <- function(df, var){
  # Discretize the variable.
  df.discr_vals <- var_discretization(df, var)
  
  # Find the most common class row-wise.
  df.discr_vals$most.common.class <- apply(df.discr_vals, 1, most_common_class)
    
  # For each method, find the percentage of matches.
  result_match <- t(apply(df.discr_vals, 1, do_class_match)) %>% 
    apply(2, function(x){ 100 * sum(x) / nrow(df.discr_vals)}) %>%
    round(2)
  
  
  # The table with the metrics of accuracy of discretization.
  acc_table <- kable(t(data.frame(Skutecznosc = result_match)), 
                col.names = c("Przedziałowanie według równej częstotliwość", 
                              "Przedziałowanie według równej szerokości", 
                              "Dyskretyzacja oparta na algorytmie K-średnich", 
                              "Stałe granice przedziału"), 
                row.names = FALSE, 
                caption = paste("Skuteczność wybranych metod dyskretyzacji dla zmiennej", gsub("\\.", " ",var)))
  
  # Return the table
  return(acc_table)
}


```

### Metodologia oceny skuteczności dyskretyzacji
Aby ocenić skuteczność każdej ze [wspomnianych metod](#discretization_intro), przyjęliśmy następującą metodologię.
Najpierw dokonaliśmy przedziałowania każdej obserwacji, korzystając ze wszystkich metod, a następnie wybraliśmy tę klasę, która występuje najczęściej (w przypadku tzw. "remisu" wybierana jest dowolna klasa). Następnie sprawdzaliśmy, w ilu przypadkach wynik przedziałowania każdej metody zgadzał się ze zagregowaną klasą. Tę liczbę podzieliliśmy przez liczbę wszystkich przypadków, aby uzyskać procent zgodności danej metody dyskretyzacji. Porównanie różnych metod przedziałowania zostały przedstawione poniżej
```{r binning_results1, echo=FALSE}
discretization_results <- evaluate_method(iris, "Sepal.Width")
discretization_results
```


```{r binning_results2, echo=FALSE}
discretization_results <- evaluate_method(iris, "Petal.Width")
discretization_results
```

### Wnioski dotyczące skuteczności przedziałowania
<!-- Czemu latex nie widzi referencji do binning_results1? -->
<!-- W ref{tab} wpisz etykietki, które ustawiasz jako argument funkcji kable -->
Z tabel \ref{tab:binning_results1} oraz \ref{tab:binning_results2} możemy wywnioskować, że w obu przypadkach największą skutecznością charakteryzuje się metoda dyskretyzacji oparta na **algorytmie K-średnich**. Z kolei najgorszą skuteczność przedziałowania obserwujemy dla metody opartej na **stałych granicach** przedziału.
Wyniki dyskretyzacji zastosowanej dla zmiennej *Petal.Width* znaczącą rożnią się od wyników przedziałowania zastosowanego dla atrybutu *Sepal.Width*. 
Jest to zgodne z intuicją — jak wykazaliśmy [wcześniej](#discriminative_power_compare), najgorsze zdolności separacyjne klas wykazuje właśnie zmienna **Sepal.Width**, co znacząco wpływa na niską skuteczność metod przedziałowania. Analogiczna zależność występuje w przypadku cechy **Petal.Width**, która z kolei charakteryzowała się wysokimi zdolnościami dyskryminacyjnymi, co przełożyło się na wysoką dokładność podejść dyskretyzacji. 

# Analiza składowych głównych
```{r new_dataset_loading, include=FALSE}
# Loading the new dataset.
CQL_df <- read.csv(file = "uaScoresDataFrame.csv") %>%
            mutate(X = NULL)

# Wybierz same cechy ilościowe.
num.cols_CQL <- colnames(select(CQL_df, where(is.numeric)))

```

## Porównanie wariancji zmiennych ilościowych.
W celu porównania wariancji wszystkich zmiennych ilościowych ze zbioru *uaScoresDataFrame*, posłużymy się wykresami pudełkowymi.
```{r variance_comparison_before, echo=FALSE, fig.cap="Wykresy pudelkowe zmiennych ciągłych przed zastosowaniem standaryzacją",fig.width = 9, fig.height  = 8}
CQL_df_longer <- CQL_df %>% pivot_longer( num.cols_CQL, values_to = "Realizacja", names_to = "Zmienna")


ggplot(CQL_df_longer, aes(y = Realizacja, fill = Zmienna)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykres pudełkowy zmiennych ilościowych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL))

```
Przyjrzyjmy się wykresowi \ref{fig:variance_comparison_before}. Obserwujemy wysokie zróżnicowanie wariancji. Z jednej strony w badanym zbiorze występują cechy o niskiej dewiacji, która charakteryzuje chociażby zmienną *Commute*. Z drugiej obecność takich zmiennych jak *Environmental.Quality* i *Venture.Capital* pokazują, że nie brakuję atrybutów o wysokiej wariancji. W celu ujednolicenia wariancji zmiennych, konieczne będzie zastosowanie standaryzacji.

```{r, feature standarization, echo = FALSE}

df_scaled <- apply(CQL_df[, num.cols_CQL], 
                   MARGIN = 2, 
                   FUN = function(x) {(x - mean(x))/sd(x)}
)

CQL_df_scaled <- as.data.frame(CQL_df)
CQL_df_scaled[, num.cols_CQL] <- df_scaled

```


Na poniższym wykresie \ref{fig:variance_comparison_after} pudełkowym widoczne są efekty standaryzacji zastosowane dla zmiennych ze zbioru danych.

```{r variance_comparison_after, echo=FALSE, fig.cap="Wykresy pudelkowe zmiennych ciągłych po zastosowaniu standaryzacji", fig.width = 9, fig.height  = 8}
CQL.df_scaled.longer <- CQL_df_scaled %>% pivot_longer( num.cols_CQL, values_to = "Realizacja", names_to = "Zmienna")


ggplot(CQL.df_scaled.longer, aes(y = Realizacja, fill = Zmienna)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykres pudełkowy zmiennych ilościowych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL))

```
## Badanie korelacji między zmiennymi.
Po dokonaniu standaryzacji zmiennych ilościowych, zbadamy jeszcze, jak silne są korelacje między atrybutami w zbiorze danych. Występowanie silnej korelacji świadczy o występowaniu reduntantnych zmiennych. Taka redundancja może zostać wyelimonowana za pomocą analizy składowych głównych. Aby poprawić czytelność wykresu, nazwy zmiennych zostały pominięte, a wartość współczynnika została przeskalowana do przedzialu [0;1].

<!-- Rozmiary wykresu korelacji do poprawy, bo tytułu nie widać  -->
```{r corr matrix, echo = FALSE, fig.cap = "Macierz korelacji dla zmiennych ciągłych"}

corr_matrix <- abs(cor(x = CQL_df[num.cols_CQL], use = "complete.obs"))

corrplot(corr_matrix, method = "square", type = "full", tl.cex = 0.8,
         title = "Macierz korelacji dla zmiennych ciągłych", tl.pos = FALSE,
         col.lim = c(0,1))
```

Na podstawie rysunku \ref{fig:corr matrix} można zauważyć, że w zdecydowanej większości przypadków korelacje pomiędzy zmiennymi są stosunkowo słabe. Niemniej jednak, występują również przypadki skrajne, w których wartości współczynnika korelacji — rozpatrywane w sensie bezwzględnym — zbliżają się do jedności, wskazując na silne liniowe powiązania między wybranymi zmiennymi. W związku z tym należy oczekiwać, że redukcja wymiarowości będzie wymagała uwzględnienia relatywnie dużej liczby składowych głównych, aby osiągnąć zakładaną frakcję wyjaśnianej wariancji.

## Wyznaczanie składowych głównych.
Dla analizowanego zbioru zmiennych ciągłych zostanie przeprowadzona analiza głównych składowych. W jej ramach porównany zostanie rozrzut składowych oraz stopień, w jakim wyjaśniają one całkowitą wariancję danych. Na zakończenie, na podstawie skumulowanej wariancji wyjaśnianej przez kolejne składowe, wyznaczona zostanie minimalna liczba komponentów niezbędnych do osiągnięcia poziomu co najmniej 80% lub 90% całkowitej wariancji.

```{r principal components computation, echo = FALSE}
prin.comp <- prcomp(x = CQL_df[num.cols_CQL], retx = TRUE) # Wyznacz składowe główne.

PCA_loadings <- prin.comp$rotation # Wyznacz ładunki składowych głównych.
PCA_variances <- (prin.comp$sdev)^2 # Znajdź wariancje składowych głównych.
PCA_rotated.df <- as.data.frame(prin.comp$x) # Wyznacz wartości składowych głównych.

# Przekształć rotated.df to postaci długiej (long form)
rotated.df_long <- pivot_longer(PCA_rotated.df,cols = everything(), names_to = "Składowa", values_to = "Wartość") 

# Liczba składowych.
n.PCA <- ncol(PCA_rotated.df)
levels.name <- paste(c("PC"), (1:n.PCA), sep = "")
levels.name <- factor(x = levels.name, 
                      levels = levels.name,
                      ordered = TRUE)

rotated.df_long$Składowa <- factor(x = rotated.df_long$Składowa,
                                   levels = levels.name, 
                                   ordered = TRUE)


```

```{r PC variance visualization, echo = FALSE, fig.height =8, fig.width = 9, fig.cap = "Wykresy pudełkowe dla składowych głównych"}
ggplot(rotated.df_long, aes(y = Wartość, fill = Składowa)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykresy pudełkowe składowych głównych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL)) + theme_minimal()



```

Mając już wyznaczone składowe główne, możemy odpowiedzieć na pytanie o minimalną liczbę komponentów niezbędnych do osiągnięcia założonej frakcji wyjaśnianej wariancji. W celu ilustracji udziału poszczególnych składowych w ogólnej wariancji danych, odwołajmy się do wykresu \ref{fig:variances explained plot}. Największe przyrosty wariancji obserwowane są dla pierwszych czterech składowych głównych, po czym tempo wzrostu wyraźnie spowalnia. W związku z tym, wstępna analiza sugeruje, iż uwzględnienie jedynie 4–6 pierwszych składowych głównych (spośród wszystkich `r ncol(PCA_rotated.df)` może być wystarczające do uzyskania satysfakcjonującego poziomu odwzorowania struktury danych.

```{r variances explained plot, echo = FALSE, fig.height=8, fig.width = 9, fig.cap = "Porównanie udziału wariancji wyjaśnianej przez poszczególne składowe główne"}

# Tworzymy ramkę danych zawierającą odsetek wariancji wyjaśnianej przez każdą składową główną.
var_explained.df <- data.frame(
  Wyjaśniana_wariancja = PCA_variances / sum(PCA_variances),  # normalizacja wariancji
  Numer_składowej = levels.name  # numery lub etykiety składowych
)

# Tworzymy wykres słupkowy przedstawiający udział poszczególnych składowych w wyjaśnianiu całkowitej wariancji
p1 <- ggplot(var_explained.df, aes(x = Numer_składowej, y = Wyjaśniana_wariancja)) + 
  geom_bar(stat = "identity", fill = "steelblue") +  # kolorystyka słupków
  ggtitle("Udział składowych głównych w wyjaśnianiu wariancji") +  # tytuł wykresu
  ylab("Proporcja wyjaśnianej wariancji") +  # etykieta osi y
  xlab("Numer składowej głównej") +  # etykieta osi x
  theme_minimal()  # estetyczny, uproszczony motyw wykresu

# Wyświetlamy wykres
print(p1)

```


```{r cum variances explained plot, echo = FALSE, fig.height=8, fig.width = 9, fig.cap = "Skumulowana wariancja wyjaśniana przez kolejne składowe główne"}

# Tworzymy ramkę danych zawierającą skumulowaną wariancję wyjaśnianą przez kolejne składowe główne
cum.var_explained.df <- data.frame(
  Łączna_wyjaśniana_wariancja = cumsum(PCA_variances) / sum(PCA_variances),  # narastająca suma wariancji
  Numer_składowej = levels.name  # etykiety składowych głównych
)

# Wykres słupkowy przedstawiający skumulowaną (narastającą) wariancję wyjaśnianą przez składowe
p2 <- ggplot(cum.var_explained.df, aes(x = Numer_składowej, y = Łączna_wyjaśniana_wariancja)) +
  geom_col(fill = "steelblue") +  # słupki w odcieniu niebieskim
  ggtitle("Skumulowana wariancja wyjaśniana przez składowe główne") +  # tytuł wykresu
  xlab("Numer składowej głównej") +
  ylab("Skumulowana proporcja wyjaśnianej wariancji") +
  scale_y_continuous(limits = c(0, 1)) +  # zakres osi y ograniczony do [0, 1]
  
  # Pozioma linia pomocnicza dla 80% wariancji
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "green", linewidth = 1) +
  annotate("text", x = 1, y = 0.78, label = "80%", color = "green", hjust = 0) +
  
  # Pozioma linia pomocnicza dla 90% wariancji
  geom_hline(yintercept = 0.9, linetype = "dashed", color = "red", linewidth = 1) +
  annotate("text", x = 1, y = 0.88, label = "90%", color = "red", hjust = 0) +
  
  theme_minimal()  # estetyczny, uproszczony motyw wykresu

# Wyświetlenie wykresu
print(p2)
```
Na podstawie wykresu \ref{fig:cum variances explained plot} możemy określić liczbę składowych głównych niezbędnych do osiągnięcia założonego poziomu wyjaśnianej wariancji. W celu odtworzenia 80% całkowitej wariancji zmiennych oryginalnych wystarczające okazuje się uwzględnienie pierwszych sześciu składowych głównych. Natomiast osiągnięcie progu 90% wymaga rozszerzenia tego zbioru do dziewięciu składowych.

## Wizualizacja danych wielowymiarowych
Po przeprowadzeniu redukcji wymiarowości za pomocą analizy głównych składowych (PCA), dokonano wizualizacji danych w przestrzeni wyznaczonej przez dwie pierwsze składowe, które kumulatywnie wyjaśniają największą część zmienności w zbiorze. Celem tej analizy jest identyfikacja potencjalnych struktur skupiskowych wśród obserwacji (miast) oraz wykrycie ewentualnych obserwacji odstających, które znacząco odbiegają od pozostałych pod względem analizowanych cech.
```{r PCA visualization, echo = FALSE, fig.height = 8, fig.width = 9, fig.cap = "Porównanie udziału wariancji wyjaśnianej przez poszczególne składowe główne bez standaryzacji"}

# Dane PCA + etykiety
PCA_data <- as.data.frame(prin.comp$x)
PCA_data$UA_Name <- CQL_df$UA_Name
PCA_data$UA_Continent <- CQL_df$UA_Continent  # grupowanie

# Dane dla zmiennych (strzałki)
PCA_vars <- as.data.frame(prin.comp$rotation)
scale_factor <- 7

# Wykres
ggplot(PCA_data, aes(x = PC1, y = PC2, color = UA_Continent)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_text_repel(aes(label = UA_Name),
                  size = 2,
                  color = "black",     # czarny tekst niezależnie od koloru grupy
                  segment.color = NA, max.overlaps = 30) +
  geom_segment(data = PCA_vars,
               aes(x = 0, y = 0,
                   xend = PC1 * scale_factor,
                   yend = PC2 * scale_factor),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "red") +
  geom_text_repel(data = PCA_vars,
                  aes(x = PC1 * scale_factor, y = PC2 * scale_factor,
                      label = rownames(PCA_vars)),
                  color = "red", size = 6, max.overlaps = 300) +
  theme_minimal() +
  labs(title = "Wizualizacja składowych głównych",
       x = "PC1", y = "PC2", color = "Continent")



```
Większość miast koncentruje się w centralnej części wykresu, co sugeruje znaczny stopień podobieństwa między nimi pod względem analizowanych cech. Mimo to, zauważalne są wyraźnie wyodrębnione grupy miast, które odbiegają od głównego skupiska. W lewym górnym obszarze wykresu wyróżniają się m.in. Londyn, Nowy Jork, Los Angeles czy Chicago – miasta o szczególnym znaczeniu globalnym. Są to zarówno stolice silnych gospodarek (np. Berlin, Londyn), jak i wiodące centra biznesowe i kulturowe (np. Nowy Jork, Los Angeles). Charakterystyczny dla tej grupy jest wysoki poziom rozwoju infrastruktury i aktywności związanych z kulturą czasu wolnego (Leisure Culture) oraz intensywna obecność kapitału wysokiego ryzyka (Venture Capital). 

Drugą wyraźnie wyodrębnioną grupę stanowią miasta zlokalizowane w prawym górnym obszarze wykresu, takie jak Delhi, Pekin, Meksyk czy Bengaluru. Są to dynamicznie rozwijające się metropolie o rosnącym znaczeniu finansowym, technologicznym oraz kulturowym. W odróżnieniu od wcześniej wspomnianych globalnych centrów, charakteryzują się one relatywnie niskimi kosztami życia, co stanowi istotny czynnik przyciągający zarówno mieszkańców, jak i inwestorów oraz przedsiębiorców poszukujących korzystnych warunków do rozwoju działalności.


## PCA bez uwzględnienia standaryzacji.
W dalszej części analizy porównana zostanie efektywność składowych głównych wyznaczonych na podstawie niestandaryzowanych zmiennych. Celem jest ocena, czy liczba komponentów niezbędnych do osiągnięcia z góry określonego progu wyjaśnionej wariancji istotnie różni się od tej uzyskanej przy zastosowaniu analizy głównych składowych z uprzednią standaryzacją danych.
```{r PCA variances unstd, echo = FALSE, fig.width = 9, fig.height = 8, fig.cap = "Wykresy pudełkowe dla składowych głównych bez zastosowanej standaryzacji"}
princ.comp_unstd <- prcomp(x = CQL_df[num.cols_CQL])

PCA_variances_unstd <- (princ.comp_unstd$sdev)^2
PCA_rotated.df_unstd <- as.data.frame(princ.comp_unstd$x)

PCA_rotated.df_longer_unstd <- pivot_longer(PCA_rotated.df_unstd,
                                            cols = everything(),
                                            values_to = "Wartość",
                                            names_to = "Składowa"
                                            )

PCA_rotated.df_longer_unstd$Składowa <- factor(x = PCA_rotated.df_longer_unstd$Składowa,
                                   levels = levels.name, 
                                   ordered = TRUE)


ggplot(PCA_rotated.df_longer_unstd, aes(y = Wartość, fill = Składowa)) +
  geom_boxplot(position=position_dodge(1)) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_blank()) +
  labs(title = "Wykresy pudełkowe składowych głównych",
       y = "Wartości")+
  guides(fill = guide_legend(title = NULL)) + theme_minimal()


```


```{r cum variances explained plot unstd, echo=FALSE, fig.height = 8, fig.width = 9,fig.cap = "Skumulowana wariancja wyjaśniana przez kolejne składowe główne"}
# Wyznacz łączna wyjaśnianą wariancję przez kolejne składowe.
cum.var_unstd_explained.df <- data.frame(
  Łączna_wyjaśniana_wariancja = cumsum(PCA_variances_unstd) / sum(PCA_variances_unstd),  # narastająca suma wariancji
  Numer_składowej = levels.name  # etykiety składowych głównych
)


# Wykres słupkowy przedstawiający skumulowaną (narastającą) wariancję wyjaśnianą przez składowe
p2 <- ggplot(cum.var_unstd_explained.df, aes(x = Numer_składowej, y = Łączna_wyjaśniana_wariancja)) +
  geom_col(fill = "steelblue") +  # słupki w odcieniu niebieskim
  ggtitle("Skumulowana wariancja wyjaśniana przez składowe główne") +  # tytuł wykresu
  xlab("Numer składowej głównej") +
  ylab("Skumulowana proporcja wyjaśnianej wariancji") +
  scale_y_continuous(limits = c(0, 1)) +  # zakres osi y ograniczony do [0, 1]
  
  # Pozioma linia pomocnicza dla 80% wariancji
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "green", linewidth = 1) +
  annotate("text", x = 1, y = 0.78, label = "80%", color = "green", hjust = 0) +
  
  # Pozioma linia pomocnicza dla 90% wariancji
  geom_hline(yintercept = 0.9, linetype = "dashed", color = "red", linewidth = 1) +
  annotate("text", x = 1, y = 0.88, label = "90%", color = "red", hjust = 0) +
  
  theme_minimal()  # estetyczny, uproszczony motyw wykresu

# Wyświetlenie wykresu
print(p2)
```
Wyniki przedstawione na wykresach \ref{fig:PCA variances unstd} oraz \ref{fig:cum variances explained plot unstd} wskazują, że brak standaryzacji danych nie wpływa istotnie na liczbę głównych składowych niezbędnych do osiągnięcia zadowalającego poziomu wyjaśnionej wariancji. Rozkład wariancji poszczególnych składowych nie odbiega w sposób znaczący od obserwowanego na wykresie \ref{fig:PC variance visualization}, gdzie uprzednio zastosowano standaryzację danych.


## Skalowanie wielowymiarowe (Multidimentional Scaling (MDS))
```{r titanic_dataset_loading, include=FALSE}
library(titanic)
titanic_data <- titanic_train
# sprawdzenie, czy typy zmiennych są poprawnie przypisane
str(titanic_data)

# zamiana na typ factor
convert <- c("Survived","Sex")
titanic_data[convert] <- lapply(titanic_data[convert], as.factor)
titanic_data$Pclass <- factor(titanic_data$Pclass, ordered=TRUE, levels=c(3,2,1))
# najczęstszy port
most_common <- names(sort(table(titanic_train$Embarked), decreasing = TRUE))[1]
# zamiana braku danych, na najczęściej występujący port i zmiana typu danych na factor
titanic_data <- titanic_data %>%
  mutate(Embarked = na_if(Embarked, "")) %>%
  mutate(Embarked = replace_na(Embarked, most_common)) %>%
  mutate(Embarked = as.factor(Embarked))

# usunięcie danych identyfikujących pasażerów
identifing <- c("PassengerId", "Name", "Ticket", "Cabin")
titanic_data <- titanic_data %>%
  select(-all_of(intersect(names(titanic_data), identifing)))

# ponowne sprawdzenie
str(titanic_data)

```

## Redukcja wymiaru na bazie MDS
```{r reduction, echo=FALSE}
library(cluster)

titanic_dissim <- titanic_data[, -1]  

# Macierz odmienności
dissim_matrix <- daisy(titanic_dissim)

# Zamiana na zwykłą macierz
dmat <- as.matrix(dissim_matrix)


```