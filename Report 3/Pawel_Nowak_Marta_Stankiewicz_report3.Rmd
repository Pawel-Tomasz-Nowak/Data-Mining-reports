---
title: "Sprawozdanie z listy 3"
subtitle: "Eksploracja danych"
author: "Marta Stankiewicz (282244)  \n Paweł Nowak (282223)"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage[OT4]{polski}
   - \usepackage[utf8]{inputenc}
   - \usepackage{graphicx}
   - \usepackage{float}
output: 
  pdf_document:
    toc: true
    fig_caption: true
    fig_width: 5 
    fig_height: 4 
    number_sections: true
fontsize: 12pt 
lof: true
lot: true


---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, include = FALSE) 
knitr::opts_chunk$set(fig.pos = "H", out.extra = '', fig.align = "center",
                      fig.height = 4, fig.width =6)
knitr::opts_chunk$set(dev.args = list(encoding = "CP1250.ENC"))

```

```{r libraries importing}
# Import necessary librarries
library(datasets)
library(caret)
library(reshape2)
library(ggplot2)
library(rlang)
library(dplyr)
library(tidyr)
library(class)
library(rpart)
library(e1071)

```

```{r, iris df loading, }
iris_df <- iris
```
```{r iris df splitting}
split_ratio <- 0.7

train_indices <- sample(seq_len(nrow(iris_df)), size = split_ratio * nrow(iris_df))

train_data <- iris_df[train_indices, ]
test_data  <- iris_df[-train_indices, ]
```


# Klasyfikacja na bazie modelu regresji liniowej
Aby ocenić skuteczność klasyfikatora opartego na modelu regresji liniowej, wykorzystamy zbiór danych iris, w którym zmienną objaśnianą jest Species, zawierająca `r length(unique(iris_df$Species))` unikalnych klas. W celu uniknięcia problemu wycieku danych (ang. data leakage), przeprowadzimy podział oryginalnego zbioru na zbiór treningowy oraz zbiór testowy, przy czym zbiory te będą zawierały odpowiednio `r round(100*split_ratio,0)` obserwacji z danych iris. Po wytrenowaniu modelu na danych treningowych, przeprowadzimy ewaluację jego skuteczności na podstawie zbioru testowego. Wyniki klasyfikacji zaprezentujemy za pomocą znormalizowanej macierzy pomyłek, w której wartości w każdej kolumnie zostaną podzielone przez sumę elementów tej kolumny, co pozwoli na lepszą interpretację skuteczności klasyfikacji dla poszczególnych klas.

```{r iris df linear regression bulding}
# Extract class labels
classes <- unique(iris_df$Species)

predictors <- c("Sepal.Length","Sepal.Width", "Petal.Length", "Petal.Width")

# Function to fit one-vs-rest linear regression model for a given class
model_fitter <- function(train_data, cls) {
  binary_response <- ifelse(train_data$Species == cls, 1, 0)
  lm(binary_response ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
     data = train_data)
}

# Train a model for each class
models <- lapply(classes, function(cls) model_fitter(train_data, cls))

# Function to predict class with the highest predicted value
predict_multiclass <- function(data, models) {
  preds <- sapply(models, function(model) predict(model, newdata = data))
  preds <- as.matrix(preds)
  colnames(preds) <- classes

  max_indices <- apply(preds, 1, which.max)
  predicted_classes <- factor(classes[max_indices], levels = classes)

  return(predicted_classes)
}

```

```{r iris df linear regression predicting}
# Predict ondata
predicted.classes <- predict_multiclass(iris_df, models)

iris_df$Predicted.Species <- predicted.classes
```


```{r confussion matrix for training samples, fig.cap = "Macierz pomyłek regresji liniowej dla obserwacji treningowych", include = TRUE}
# Find the true labels
G <- iris_df[train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek regresji liniowej dla próbek treningowych",
       x = "Klasa prognozowana", y = "Rzeczywista klasa", fill = "Precision") +
  theme_minimal()

```
## Analiza skuteczności klasyfikacji dla zbioru treningowego
Na podstawie macierzy pomyłek przedstawionej na Rysunku \ref{fig:confussion matrix for training samples}, przyjrzeliśmy się, jak dobrze klasyfikator oparty na regresji liniowej radzi sobie z przewidywaniem poszczególnych klas. Zauważyliśmy, że skuteczność tych przewidywań różni się w zależności od tego, do której klasy należą próbki treningowe.
Najlepiej klasyfikator poradził sobie z klasą setosa oraz virginica - dla obu tych klas precyzja wyniosła ponad 92%. To pokazuje, że model bardzo dobrze rozpoznaje te dwie klasy w zbiorze treningowym i rzadko się myli, przypisując im inne próbki.
Jednak w przypadku klasy versicolor skuteczność przewidywania wyraźnie spadła, osiągając tylko 71% precyzji. To sugeruje, że klasyfikator ma większy problem z prawidłowym rozpoznawaniem próbek należących do tej klasy.
Możemy przypuszczać, że powodem gorszych wyników dla gatunku versicolor jest tak zwany problem maskowania klasy (class masking problem). Chodzi o to, że cechy charakterystyczne dla klasy versicolor mogą być podobne do cech innych klas, co utrudnia modelowi regresji liniowej jednoznaczne przypisanie próbek do właściwej kategorii. Aby sprawdzić, czy tak jest, przeanalizujemy teraz kolejny wykres.

```{r linreg plots}
create_linreg_data <- function(class, predictor = "Sepal.Length"){
  class_idx <- which(classes == class)
  model_do_wykresu <- models[[class_idx]] # Find the model for this particular class
  wspolczynniki <- coef(model_do_wykresu)  # Find the model coefficients

  przewidywane_wartosci <- wspolczynniki[1]  + wspolczynniki[predictor] * train_data[[predictor]] # Find the prediction error
  wartosci_regresji <- data.frame("X" = train_data[[predictor]], # Create data frame
                                   "Y" = przewidywane_wartosci,
                                   "Class" = class)
  colnames(wartosci_regresji) = c(predictor, "Y", "Class") # Set the column names
  return(wartosci_regresji)
}
```

```{r, class masking problem, fig.cap = "Krzywe regresji liniowej dla różnych gatunków kwiatów", include = TRUE}
predictor <- "Sepal.Width" # Wybiera predyktor

dfs <- lapply(classes, function(x) {create_linreg_data(x, predictor)}) # Tworzy listę ramek danych
combined_df <- bind_rows(dfs) # Łączy ramki danych

# Tworzy wykres liniowy
p1 <- ggplot(combined_df, aes(x = !!sym(predictor), y = Y, color = Class)) +
  geom_line(size = 1) +
  labs(title = "Krzywe regresji liniowej dla różnych gatunków kwiatów",
       x = gsub("\\.", " ", predictor), # Etykieta osi X
       y = "Prognoza", # Etykieta osi Y
       color = "Class") # Etykieta legendy
print(p1)
```
Analiza przedstawionych na rysunku \ref{fig:class masking problem} prostych regresji liniowych ujawnia problem maskowania klas w odniesieniu do kategorii 'Versicolor'. W obszarze niskich wartości predyktora `r predictor`, charakteryzujących się największym prawdopodobieństwem obserwacji, krzywa regresji odpowiadająca gatunkowi Iris versicolor przebiega pomiędzy krzywymi pozostałych klas. Taka konfiguracja przestrzenna implikuje, iż w zakresie wspomnianych wartości predyktora, klasyfikator oparty na bezpośrednim porównaniu wartości regresji liniowej może systematycznie pomijać przynależność obserwacji do klasy 'Versicolor', prowadząc do potencjalnych błędów klasyfikacji.

## Analiza skuteczności klasyfikacji dla zbioru testowego
```{r confussion matrix for testing samples, fig.cap = "Macierz pomyłek regresji liniowej dla obserwacji testowych", include = TRUE}
# Find the true labels
G <- iris_df[-train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[-train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek testowych",
       x = "Predicted Class", y = "True Class", fill = "Precision") +
  theme(legend.position = "none") +
  theme_minimal()
```
Podobne wnioski można wyciągnąć z oceny skuteczności modelu regresji na zbiorze testowym, co ilustruje macierz pomyłek na rysunku \ref{fig:confussion matrix for testing samples}. Klasa 'versicolor' wykazuje relatywnie wysoką częstotliwość błędnych klasyfikacji, co jest prawdopodobnie konsekwencją wspomnianego problemu maskowania klas.

# Klasyfikacja na bazie modelu regresji liniowej z czynnikami wielomianowymi
Trudności związane z maskowaniem klas znacząco utrudniają stworzenie efektywnego klasyfikatora opartego na modelu regresji liniowej. W celu zminimalizowania tego problemu i poprawy jakości klasyfikacji, wykorzystamy predyktory do wygenerowania czynników wielomianowych, czyli wyrażeń w formie: ...
$$
\begin{aligned}
& X_{1}^{t_{1}} X_{2}^{t_{2}} \dots X_{p}^{t_{p}}, \\
& \text{gdzie } \sum_{i=1}^{p} t_{i} = 2 \quad \text{oraz} \quad \forall i \in \{1, \dots, p\} \quad t_{i} \geq 0
\end{aligned}
$$
```{r, polynomial features creating}

# Creating the polynomial features
iris_df$Sepal.Length2 <- iris_df$Sepal.Length^2
iris_df$Sepal.Width2 <- iris_df$Sepal.Width^2
iris_df$Petal.Length2 <- iris_df$Petal.Length^2
iris_df$Petal.Width2 <- iris_df$Petal.Width^2

iris_df$Sepal.Length_Sepal.Width <- iris_df$Sepal.Length * iris_df$Sepal.Width
iris_df$Sepal.Length_Petal.Length <- iris_df$Sepal.Length * iris_df$Petal.Length
iris_df$Sepal.Length_Petal.Width <- iris_df$Sepal.Length * iris_df$Petal.Width
iris_df$Sepal.Width_Petal.Length <- iris_df$Sepal.Width * iris_df$Petal.Length
iris_df$Sepal.Width_Petal.Width <- iris_df$Sepal.Width * iris_df$Petal.Width
iris_df$Petal.Length_Petal.Width <- iris_df$Petal.Length * iris_df$Petal.Width

```

```{r, poly iris df splitting}
train_indices <- sample(seq_len(nrow(iris_df)), size = split_ratio * nrow(iris_df))

iris_df <-   iris_df %>% mutate(Predicted.Species = NULL)

train_data <- iris_df[train_indices, ]
test_data <- iris_df[-train_indices, ]

```

```{r, poly linear regression building}
# Train a model for each class

# Function to fit one-vs-rest linear regression model for a given class


model_fitter <- function(train_data, cls) {
  binary_response <- ifelse(train_data$Species == cls, 1, 0)
  lm(binary_response ~ ., data = train_data[, names(train_data) != "Species"])
}

models <- lapply(classes, function(cls) model_fitter(train_data, cls))


```

```{r iris df poly linear regression predicting}
# Predict on data
predicted.classes <- predict_multiclass(iris_df, models)

iris_df$Predicted.Species <- predicted.classes

```


```{r confussion matrix for training samples with polynomial features, fig.cap = "Macierz pomyłek regresji liniowej z cechami wielomianowymi", include = TRUE}
# Find the true labels
G <- iris_df[train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek treningowych",
       subtitle = "Z czynnikami wielomianowymi",
       x = "Predicted Class", y = "True Class", fill = "Precision") +
  theme_minimal()

```



```{r confussion matrix for testing samples with polynonomial features, fig.cap = "Macierz pomyłek regresji liniowej z cechami wielomianowymi", include = TRUE}
# Find the true labels
G <- iris_df[-train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[-train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek testowych",
       subtitle  = "Z czynnikami wielomianowymi",
       x = "Prognozowana klasa", y = "Rzeczywista klasa", fill = "Precision") +
  theme_minimal()

```
## Wnioski
Analiza map cieplnych macierzy pomyłek wykazała znaczącą poprawę skuteczności klasyfikacji po zastosowaniu modelu regresji liniowej z uwzględnieniem cech wielomianowych. W odróżnieniu od modelu bazowego, w którym zaobserwowano problem maskowania klasy oraz niską skuteczność klasyfikacji próbek należących do klasy 'versicolor', rozszerzony model z cechami wielomianowymi charakteryzuje się niemal całkowitym wyeliminowaniem tych niekorzystnych zjawisk.

Wykresy macierzy pomyłek jednoznacznie wskazują, że wprowadzenie czynników wielomianowych przyczyniło się do lepszego rozdzielenia przestrzeni cech, co w konsekwencji umożliwiło modelowi regresji liniowej dokładniejsze przypisanie próbek do właściwych klas. Zanik "pomijania" klasy 'versicolor' oraz ogólnie wyższa koncentracja wartości na głównej diagonali macierzy pomyłek dla modelu z cechami wielomianowymi stanowią silne argumenty przemawiające za istotnością rozszerzenia zestawu cech o komponenty wielomianowe.

Na podstawie przeprowadzonych obserwacji można zatem wnioskować, że dodanie czynników wielomianowych do modelu regresji liniowej jest uzasadnione i korzystnie wpływa na zdolności klasyfikacyjne modelu w analizowanym problemie. Rozszerzenie przestrzeni cech o interakcje i potęgi oryginalnych cech dostarcza modelowi dodatkowych informacji, które pozwalają na tworzenie bardziej złożonych i dokładnych granic decyzyjnych między klasami.

# Porównanie metod klasyfikacji

```{r, pimaindiansdiabetes loading}
library("mlbench")
data("PimaIndiansDiabetes2")
pimaindians.df <- PimaIndiansDiabetes2

```
W tym rozdziale skupimy się na porównaniu wybranych metod klasyfikacyjnych: algorytmu K-najbliższych sąsiadów, drzewa decyzyjnego oraz klasyfikatora bayesowskiego. Reguły decyzyjne zostaną wytrenowane i przetestowane na zbiorze danych PimaIndiansDiabetes2, który jest dostępny w pakiecie mlbench w języku R. Przed przystąpieniem do budowy modeli podzielimy dane na zbiór treningowy i testowy, aby zapobiec zjawisku wycieku danych oraz zapewnić wiarygodną ocenę skuteczności klasyfikatorów. 

Zbiór danych PimaIndiansDiabetes2 zawiera `r nrow(pimaindians.df)` przypadków oraz `r ncol(pimaindians.df)` zmiennych, z czego `r ncol(pimaindians.df)-1` stanowi potencjalne cechy predykcyjne, a jedna kolumna – diabetes – pełni rolę zmiennej objaśnianej. Określa ona, czy dana osoba – kobieta pochodząca z rdzennego plemienia Pima, zamieszkującego stan Arizona w USA – należy do grupy osób chorujących na cukrzycę typu 2. W celu uproszczenia analizy, poziomy zmiennej diabetes zostały zmienione z "pos" i "neg" na "1" i "0", przy czym zmienna zachowała typ czynnika (ang. factor).



```{r target var transforming}
pimaindians.df  <-  pimaindians.df %>% mutate(diabetes = ifelse(pimaindians.df$diabetes == "pos", 1, 0))

pimaindians.df$diabetes <- factor(pimaindians.df$diabetes)
```
Wstępna analiza opisowa zbioru danych wykazała, że brakujące wartości zostały oznaczone w sposób zgodny z powszechną konwencją, czyli przy użyciu symbolu NA. W danych nie występują nietypowe lub niepoprawne sposoby kodowania braków, takie jak wartość 0 w kolumnie insulin, co czasem spotyka się w gorzej przygotowanych zbiorach. Zmienna docelowa diabetes jest prawidłowo przechowywana jako czynnik (ang. factor), co umożliwia bezpośrednie jej wykorzystanie w zadaniach klasyfikacyjnych.

## Wstępna analiza danych.
Na początku przeprowadzona zostanie analiza rozkładu klas zmiennej docelowej. Jest to ważny krok, gdyż nierównomierne rozłożenie klas (tzw. problem niezbalansowanych danych) może znacząco wpłynąć na ocenę skuteczności modeli.
```{r, target classes distribution, fig.cap = "Rozkład etykiet zmiennej celu diabetes", include = TRUE}
# Probability for each class of `diabetes` target variable.
probs <- pimaindians.df %>%
  group_by(diabetes) %>%
  summarise(liczba = n()) %>%
  mutate(prawdopodobienstwo = liczba / sum(liczba))

probs$diabetes <- factor(probs$diabetes)

ggplot(probs, aes(x = diabetes, y = prawdopodobienstwo, fill = diabetes)) +
  geom_bar(stat = "identity") +
  labs(x = "Klasa", y = "Odsetek", title = "Rozkład etykiet zmiennej `Diabetes`")

```
Z rysunku \ref{fig:target classes distribution} wyraźnie wynika, że mamy do czynienia z problemem niezbalansowanych danych — liczba osób niechorujących na cukrzycę typu 2 jest niemal dwukrotnie większa niż liczba osób chorych. Teoretycznie, gdybyśmy zastosowali prosty, „naiwny” klasyfikator przypisujący każdą obserwację do klasy dominującej, uzyskalibyśmy wysoką ogólną skuteczność. Jednak po bliższej analizie metryk oceniających dokładność dla obu klas okazałoby się, że taki model w praktyce jest nieskuteczny, ponieważ całkowity błąd klasyfikacji dla klasy mniejszościowej wyniósłby 100%.

Przeanalizujemy teraz rozkłady ciągłych predyktorów. Jest to bardzo istotna kwestia, zwłaszcza w kontekście klasyfikatora KNN, gdzie różne skale pomiarowe różnych zmiennych mogą znacząco zanizać wpływ zmiennych charakteryzujących się zwięzłym zakresem wartości.

```{r variance comparison, fig.cap = "Porównanie wariancji predyktorów", include = TRUE}
# Find predictors
predictors <- setdiff(colnames(pimaindians.df), "diabetes")

# Dataframe with predictors
X <- pimaindians.df[predictors]

# Convert to long format
X.long <- X %>%
  pivot_longer(cols = everything(),
               names_to = "predictor",
               values_to = "realization")

# Plotting boxplots
ggplot(X.long, aes(x = predictor, y = realization, fill = predictor)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # rotate x-axis labels 45 degrees for better readability
    legend.position = "none"                             # hide legend (fill color is redundant with x-axis labels)
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów"               # tytuł wykresu
  )

```
Z rysunku \ref{fig:variance comparison} jasno wynika, że standaryzacja predyktorów jest niezbędna. Zmienne różnią się istotnie zarówno pod względem wariancji, jak i wartości tendencji centralnej, co szczególnie widoczne jest na przykładzie porównania wykresów pudełkowych zmiennych „insulin” oraz „triceps”. Dokonajmy zatem standaryzacji i spójrzmy na rezultaty, które przedstawiono na rysunku \ref{fig:variance comparison after standarization}

```{r variance comparison after standarization, fig.cap = "Porównanie wariancji predyktorów po zastosowaniu standaryzacji", include = TRUE}

X.std <- data.frame(apply(X, 2, function(x){ (x - mean(x, na.rm = TRUE))/ sd(x, na.rm = TRUE)}
))

# Convert to long format
X.std.long <- X.std %>%
  pivot_longer(cols = everything(),
               names_to = "predictor",
               values_to = "realization")


# Plotting boxplots
ggplot(X.std.long, aes(x = predictor, y = realization, fill = predictor)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # rotate x-axis labels 45 degrees for better readability
    legend.position = "none"                             # hide legend (fill color is redundant with x-axis labels)
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów",               # tytuł wykresu
    subtitle = "Po zastosowaniu standaryzacji"            # Podtytuł wykresu
  )


```
Mając już ujednoliconą skalę pomiarową dla wszystkich predyktorów, możemy przejść do oceny ich zdolności dyskryminacyjnej. Ten etap pozwoli nam zidentyfikować zmienne najlepiej rozróżniające klasy i lepiej zrozumieć ich wpływ na działanie modelu.
```{r dyscrimination power analysis, include = TRUE, fig.width= 7, fig.height = 6, fig.cap = "Porównanie zdolności dyskryminacyjnych predyktorów"}
# Apply the standarization to the dataframe
pimaindians.df[predictors] = X.std


pimaindians.long.df <- pimaindians.df %>% pivot_longer( cols = where(is.numeric), values_to = "realization", 
                                                        names_to = "predictor")

# Plotting boxplots
ggplot(pimaindians.long.df, aes(x = predictor, y = realization, fill = diabetes)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)  # rotate x-axis labels 45 degrees for better readability
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów po zastosowaniu standaryzacji",               # tytuł wykresu
  )


```
Jak pokazano na rysunku \ref{fig:dyscrimination power analysis}, żadna ze zmiennych nie rozdziela klas docelowych w sposób idealny. Mimo to, można wyróżnić predyktory o wyraźnie silniejszych zdolnościach dyskryminacyjnych. Do takich atrybutów należą zmienne: 'glucose', 'insulin', 'triceps' oraz 'mass'. Natomiast najsłabszą zdolność separacji klas wykazują zmienne 'pedigree' oraz 'pressure'.

# Ocena dokładności klasyfikacji i porównanie metod

Celem niniejszej części analizy jest szczegółowe porównanie trzech wybranych algorytmów klasyfikacyjnych: K-najbliższych sąsiadów (KNN), naiwnego klasyfikatora Bayesowskiego oraz drzewa decyzyjnego. Aby zapewnić obiektywną i rzetelną ocenę skuteczności poszczególnych metod, wszystkie modele zostaną wytrenowane oraz przetestowane na tym samym podziale danych. Dzięki temu możliwe będzie bezpośrednie porównanie ich wyników, przy zachowaniu jednolitych warunków eksperymentu.

 
```{r dataset splitting}
split_ratio <- 0.7


# Imputation by mean
pimaindians.df <- pimaindians.df %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))


train_indices <- sample(seq_len(nrow(pimaindians.df)), size = split_ratio * nrow(pimaindians.df))

train_data <- pimaindians.df[train_indices, ]
test_data  <- pimaindians.df[-train_indices, ]


train.X <- train_data[, predictors] # Training set of predictors
train.Y <- train_data[, c("diabetes")] # Training set of target variable

test.X <- test_data[, predictors] # Testing set of predictors
test.Y <- test_data[, c("diabetes")]  # Testing set of target variable

```

```{r KNN training, include = TRUE}
# Build the KNN model
KNN_build <- function(k){
knn_pred <- knn(train = train.X, test = test.X, cl = train.Y, k = k)

return (knn_pred)
}

knn_pred <- KNN_build(5)
```

```{r classification tree, include=TRUE}
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class")
tree_pred <- predict(tree_model, test.X, type = "class")

```

```{r naive bayes classifier, include=TRUE}
nb_model <- naiveBayes(x = train.X, y = train.Y)
nb_pred <- predict(nb_model, newdata = test.X)
```

```{r confusion matrixes for algorithms, include=TRUE}

plot_confusion_matrix <- function(predictions, true_labels, model_name = "Model", subtitle = "") {
  
  predictions <- as.factor(predictions)
  true_labels <- as.factor(true_labels)
  levels(predictions) <- levels(true_labels)
  
  conf_raw <- confusionMatrix(data = predictions, reference = true_labels)
  conf_table <- conf_raw$table
  
  conf_melted <- melt(prop.table(conf_table, margin = 2))
  colnames(conf_melted) <- c("Predicted.label", "True.label", "Precision")
  
  if (all(levels(true_labels) %in% c("0", "1"))) {
    conf_melted$Predicted.label <- factor(conf_melted$Predicted.label, levels = c("0", "1"))
    conf_melted$True.label <- factor(conf_melted$True.label, levels = c("0", "1"))
  }

  p <- ggplot(conf_melted, aes(x = Predicted.label, y = True.label, fill = Precision)) +
    geom_tile(color = "white") +
    geom_text(aes(label = round(Precision, 2)), size = 4) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = paste("Macierz pomyłek -", model_name),
         subtitle = subtitle,
         x = "Prognozowana klasa", y = "Rzeczywista klasa", fill = "Precision") +
    theme_minimal()
  
  class_precisions <- filter(conf_melted, Predicted.label == True.label)
  precision.0 <- class_precisions[class_precisions$True.label == "0", "Precision"]
  precision.1 <- class_precisions[class_precisions$True.label == "1", "Precision"]

  return(list(
    plot = p,
    precision.0 = precision.0,
    precision.1 = precision.1
  ))
}


knn_result <- plot_confusion_matrix(knn_pred, test.Y, model_name = "KNN", subtitle = "k = 5")
tree_result <- plot_confusion_matrix(tree_pred, test.Y, model_name = "Drzewo decyzyjne")
nb_result   <- plot_confusion_matrix(nb_pred, test.Y, model_name = "Naiwny Bayes")

```

```{r KNN evaluating, fig.cap = "Macierz pomyłek dla algorytmu KNN, k = 5", include = TRUE}
print(knn_result$plot)
```

```{r clasification tree evaluating, fig.cap = "Macierz pomyłek dla drzewa klasyfikacyjnego", include = TRUE}
print(tree_result$plot)
```

```{r naive bayes evaluating, fig.cap = "Macierz pomyłek dla naiwnego klasyfikatora bayesa", include = TRUE}
print(nb_result $plot)
```


Jak przedstawiono na rysunku \ref{fig:KNN evaluating}, metoda KNN przy liczbie sąsiadów 'k = 5' nie osiąga zadowalających rezultatów. Wartość precyzji dla klasy 0 wynosi `r round(knn_result$precision.0,2)`, natomiast dla klasy 1 `r round(knn_result$precision.1,2)`. Uzyskane wyniki wskazują na niewystarczającą skuteczność modelu w obecnej konfiguracji, co skłania do dalszej analizy wpływu hiperparametru 'k' na jakość klasyfikacji.

W tym celu porównamy błędy klasyfikacji algorytmu dla różnych wartości 'k', najpierw stosując jednokrotny podział danych na zbiór treningowy i testowy, a następnie wykorzystując walidację krzyżową. Podejście to pozwoli na bardziej obiektywną ocenę efektywności modelu w zależności od doboru liczby sąsiadów.



```{r classification error, include=TRUE}
plot_classification_error <- function(param_values, model_function, param_name = "Parametr", model_label = "Model") {
  
  errors <- sapply(param_values, function(p) {
    pred <- model_function(p)
    mean(pred != test.Y)
  })
  
  df <- data.frame(param = param_values, CE = errors)
  best_param <- df$param[which.min(df$CE)]
  min_CE <- min(df$CE)
  
  p <- ggplot(df, aes(x = param, y = CE)) +
    geom_line(color = "blue", size = 1) +
    geom_point(color = "red", size = 1.8) +
    geom_point(data = subset(df, param == best_param), aes(x = param, y = CE), 
               color = "green", size = 2.5) +
    labs(
      title = paste("Błąd klasyfikacji w zależności od", param_name),
      subtitle = paste("Model:", model_label),
      x = param_name,
      y = "Błąd klasyfikacji"
    ) +
    scale_x_continuous(breaks = param_values[seq(1, length(param_values), by = 2)]) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11),
      axis.title = element_text(size = 12),
      axis.text = element_text(angle = 45, size = 10),
      panel.grid.minor = element_blank()
    )
  return(list(
    plot = p,
    best_param = best_param,
    min_error = min_CE,
    data = df
  ))
}


```

```{r Classification error of KNN, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji KNN dla różnych wartości hiperparametru k"}
knnfun <- plot_classification_error(
  param_values = 1:25,
  model_function = KNN_build,
  param_name = "Liczba sąsiadów (k)",
  model_label = "KNN"
)

knnfun$plot

```


```{r Classification error of the tree , include = TRUE, fig.cap = "Porównanie błędu klasyfikacji drzewa decyzyjnego dla różnych wartości parametru cp"}

Tree_build <- function(cp_value) {
  model <- rpart(diabetes ~ ., data = train_data, method = "class",
                 control = rpart.control(cp = cp_value))
  predict(model, test.X, type = "class")
}

treefun <- plot_classification_error(
  param_values = seq(0.001, 0.05, by = 0.002),
  model_function = Tree_build,
  param_name = "Parametr cp",
  model_label = "Drzewo decyzyjne"
)

treefun$plot

```
```{r Classification naive bayes , include = TRUE, fig.cap = "Porównanie błędu klasyfikacji naiwnego klasyfikatora bayesowskiego dla różnych wartości parametru laplace"}
NB_build <- function(laplace_value) {
  model <- naiveBayes(diabetes ~ ., data = train_data, laplace = laplace_value)
  predict(model, test.X)
}

nbfun <- plot_classification_error(
  param_values = 0:5,
  model_function = NB_build,
  param_name = "Laplace smoothing",
  model_label = "Naiwny klasyfikator Bayesa"
)

nbfun$plot

```


```{r cross validation, include = TRUE}
library(caret)
library(ggplot2)
library(scales)

cv_model_error_plot <- function(train.X, train.Y, method, param_name, param_values,
                                folds = 25, seed = 123, model_label = NULL) {
  
  train_data <- data.frame(X = train.X, Y = as.factor(train.Y))
  
  ctrl <- trainControl(method = "cv", number = folds)
  
  tune_grid <- data.frame(param_values)
  colnames(tune_grid) <- param_name
  
  set.seed(seed)
  model_cv <- train(Y ~ ., data = train_data,
                    method = method,
                    trControl = ctrl,
                    tuneGrid = tune_grid)
  
  results <- model_cv$results
  results$ClassificationError <- 1 - results$Accuracy
  
  best_param <- results[[param_name]][which.min(results$ClassificationError)]
  min_CE <- min(results$ClassificationError)
  
  if (is.null(model_label)) model_label <- method
  
  p <- ggplot(results, aes_string(x = param_name, y = "ClassificationError")) +
    geom_line(color = "blue") +
    geom_point(color = "red", size = 2.5) + 
    geom_point(data = subset(results, results[[param_name]] == best_param),
               aes_string(x = param_name, y = "ClassificationError"),
               color = "green", size = 2.5) +
    labs(title = paste("Błąd klasyfikacji w cross-validation dla", model_label),
         x = param_name,
         y = "Błąd klasyfikacji") +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    theme_minimal()
  
  return(list(
    best_param = best_param,
    min_error = min_CE,
    plot = p
  ))
}


```

```{r cross validation for KNN, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji KNN dla różnych wartości hiperparametru k z uwzględnieniem walidacji krzyżowej"}
result_knn <- cv_model_error_plot(train.X, train.Y,
                                 method = "knn",
                                 param_name = "k",
                                 param_values = 1:25,
                                 model_label = "KNN")

print(result_knn$plot)

```

```{r cross validation for classification tree, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji drzewa  klasyfikacyjnego dla różnych wartości parametru cp z uwzględnieniem walidacji krzyżowej"}
result_tree <- cv_model_error_plot(train.X, train.Y,
                                  method = "rpart",
                                  param_name = "cp",
                                  param_values = seq(0.001, 0.05, by = 0.005),
                                  model_label = "Drzewo decyzyjne")

print(result_tree$plot)


```

```{r cross validation for naive bayes, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji naiwnego klasyfikatora bayesa dla różnych wartości parametru laplace z uwzględnieniem walidacji krzyżowej"}

result_nb <- cv_model_error_plot(train.X, train.Y,
  method = "nb",           
  param_name = "fL",   
  param_values = seq(0, 2, by = 0.5),
  model_label = "Naive Bayes (Laplace smoothing)"
)

```

\newpage
Rysunki \ref{fig:Classification error of KNN} oraz \ref{fig:cross validation for KNN} ilustrują estymowaną wartość błędu klasyfikacji w zależności od liczby sąsiadów k w algorytmie KNN. Na podstawie wykresu dla jednokrotnego podziału zbioru danych wnioskujemy, że najniższy błąd osiągany jest dla k = \texttt{`r knnfun$best_param`}. Z kolei analiza wyników uzyskanych metodą walidacji krzyżowej wskazuje, że optymalną wartością hiperparametru jest k = \texttt{`r result_knn$best_param`}. Obserwacje te potwierdzają znaczenie odpowiedniego doboru parametru 
k oraz pokazują, że walidacja krzyżowa może dostarczyć bardziej wiarygodnych oszacowań błędu generalizacji.


