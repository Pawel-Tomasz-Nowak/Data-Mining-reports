---
title: "Sprawozdanie z listy 3"
subtitle: "Eksploracja danych"
author: "Marta Stankiewicz (282244)  \n Paweł Nowak (282223)"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage[OT4]{polski}
   - \usepackage[utf8]{inputenc}
   - \usepackage{graphicx}
   - \usepackage{float}
output: 
  pdf_document:
    toc: true
    fig_caption: true
    fig_width: 5 
    fig_height: 4 
    number_sections: true
fontsize: 12pt 
lof: true
lot: true


---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, include = FALSE) 
knitr::opts_chunk$set(fig.pos = "H", out.extra = '', fig.align = "center",
                      fig.height = 4, fig.width =6)
knitr::opts_chunk$set(dev.args = list(encoding = "CP1250.ENC"))

```

```{r libraries importing}
# Import necessary librarries
library(datasets)
library(caret)
library(reshape2)
library(ggplot2)
library(rlang)
library(dplyr)
library(tidyr)
library(class)
library(rpart)
library(e1071)

```

```{r, iris df loading, }
iris_df <- iris
```
```{r iris df splitting}
split_ratio <- 0.7

train_indices <- sample(seq_len(nrow(iris_df)), size = split_ratio * nrow(iris_df))

train_data <- iris_df[train_indices, ]
test_data  <- iris_df[-train_indices, ]
```


# Klasyfikacja na bazie modelu regresji liniowej
Aby ocenić skuteczność klasyfikatora opartego na modelu regresji liniowej, wykorzystamy zbiór danych iris, w którym zmienną objaśnianą jest Species, zawierająca `r length(unique(iris_df$Species))` unikalnych klas. W celu uniknięcia problemu wycieku danych (ang. data leakage), przeprowadzimy podział oryginalnego zbioru na zbiór treningowy oraz zbiór testowy, przy czym zbiory te będą zawierały odpowiednio `r round(100*split_ratio,0)` obserwacji z danych iris. Po wytrenowaniu modelu na danych treningowych, przeprowadzimy ewaluację jego skuteczności na podstawie zbioru testowego. Wyniki klasyfikacji zaprezentujemy za pomocą znormalizowanej macierzy pomyłek, w której wartości w każdej kolumnie zostaną podzielone przez sumę elementów tej kolumny, co pozwoli na lepszą interpretację skuteczności klasyfikacji dla poszczególnych klas.

```{r iris df linear regression bulding}
# Extract class labels
classes <- unique(iris_df$Species)

predictors <- c("Sepal.Length","Sepal.Width", "Petal.Length", "Petal.Width")

# Function to fit one-vs-rest linear regression model for a given class
model_fitter <- function(train_data, cls) {
  binary_response <- ifelse(train_data$Species == cls, 1, 0)
  lm(binary_response ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
     data = train_data)
}

# Train a model for each class
models <- lapply(classes, function(cls) model_fitter(train_data, cls))

# Function to predict class with the highest predicted value
predict_multiclass <- function(data, models) {
  preds <- sapply(models, function(model) predict(model, newdata = data))
  preds <- as.matrix(preds)
  colnames(preds) <- classes

  max_indices <- apply(preds, 1, which.max)
  predicted_classes <- factor(classes[max_indices], levels = classes)

  return(predicted_classes)
}

```

```{r iris df linear regression predicting}
# Predict ondata
predicted.classes <- predict_multiclass(iris_df, models)

iris_df$Predicted.Species <- predicted.classes
```


```{r confussion matrix for training samples, fig.cap = "Macierz pomyłek regresji liniowej dla obserwacji treningowych", include = TRUE}
# Find the true labels
G <- iris_df[train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek regresji liniowej dla próbek treningowych",
       x = "Klasa prognozowana", y = "Rzeczywista klasa", fill = "Precision") +
  theme_minimal()

```
## Analiza skuteczności klasyfikacji dla zbioru treningowego
Na podstawie macierzy pomyłek przedstawionej na Rysunku \ref{fig:confussion matrix for training samples}, przyjrzeliśmy się, jak dobrze klasyfikator oparty na regresji liniowej radzi sobie z przewidywaniem poszczególnych klas. Zauważyliśmy, że skuteczność tych przewidywań różni się w zależności od tego, do której klasy należą próbki treningowe.
Najlepiej klasyfikator poradził sobie z klasą setosa oraz virginica - dla obu tych klas precyzja wyniosła ponad 92%. To pokazuje, że model bardzo dobrze rozpoznaje te dwie klasy w zbiorze treningowym i rzadko się myli, przypisując im inne próbki.
Jednak w przypadku klasy versicolor skuteczność przewidywania wyraźnie spadła, osiągając tylko 71% precyzji. To sugeruje, że klasyfikator ma większy problem z prawidłowym rozpoznawaniem próbek należących do tej klasy.
Możemy przypuszczać, że powodem gorszych wyników dla gatunku versicolor jest tak zwany problem maskowania klasy (class masking problem). Chodzi o to, że cechy charakterystyczne dla klasy versicolor mogą być podobne do cech innych klas, co utrudnia modelowi regresji liniowej jednoznaczne przypisanie próbek do właściwej kategorii. Aby sprawdzić, czy tak jest, przeanalizujemy teraz kolejny wykres.

```{r linreg plots}
create_linreg_data <- function(class, predictor = "Sepal.Length"){
  class_idx <- which(classes == class)
  model_do_wykresu <- models[[class_idx]] # Find the model for this particular class
  wspolczynniki <- coef(model_do_wykresu)  # Find the model coefficients

  przewidywane_wartosci <- wspolczynniki[1]  + wspolczynniki[predictor] * train_data[[predictor]] # Find the prediction error
  wartosci_regresji <- data.frame("X" = train_data[[predictor]], # Create data frame
                                   "Y" = przewidywane_wartosci,
                                   "Class" = class)
  colnames(wartosci_regresji) = c(predictor, "Y", "Class") # Set the column names
  return(wartosci_regresji)
}
```

```{r, class masking problem, fig.cap = "Krzywe regresji liniowej dla różnych gatunków kwiatów", include = TRUE}
predictor <- "Sepal.Width" # Wybiera predyktor

dfs <- lapply(classes, function(x) {create_linreg_data(x, predictor)}) # Tworzy listę ramek danych
combined_df <- bind_rows(dfs) # Łączy ramki danych

# Tworzy wykres liniowy
p1 <- ggplot(combined_df, aes(x = !!sym(predictor), y = Y, color = Class)) +
  geom_line(size = 1) +
  labs(title = "Krzywe regresji liniowej dla różnych gatunków kwiatów",
       x = gsub("\\.", " ", predictor), # Etykieta osi X
       y = "Prognoza", # Etykieta osi Y
       color = "Class") # Etykieta legendy
print(p1)
```
Analiza przedstawionych na rysunku \ref{fig:class masking problem} prostych regresji liniowych ujawnia problem maskowania klas w odniesieniu do kategorii 'Versicolor'. W obszarze niskich wartości predyktora `r predictor`, charakteryzujących się największym prawdopodobieństwem obserwacji, krzywa regresji odpowiadająca gatunkowi Iris versicolor przebiega pomiędzy krzywymi pozostałych klas. Taka konfiguracja przestrzenna implikuje, iż w zakresie wspomnianych wartości predyktora, klasyfikator oparty na bezpośrednim porównaniu wartości regresji liniowej może systematycznie pomijać przynależność obserwacji do klasy 'Versicolor', prowadząc do potencjalnych błędów klasyfikacji.

## Analiza skuteczności klasyfikacji dla zbioru testowego
```{r confussion matrix for testing samples, fig.cap = "Macierz pomyłek regresji liniowej dla obserwacji testowych", include = TRUE}
# Find the true labels
G <- iris_df[-train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[-train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek testowych",
       x = "Predicted Class", y = "True Class", fill = "Precision") +
  theme(legend.position = "none") +
  theme_minimal()
```
Podobne wnioski można wyciągnąć z oceny skuteczności modelu regresji na zbiorze testowym, co ilustruje macierz pomyłek na rysunku \ref{fig:confussion matrix for testing samples}. Klasa 'versicolor' wykazuje relatywnie wysoką częstotliwość błędnych klasyfikacji, co jest prawdopodobnie konsekwencją wspomnianego problemu maskowania klas.

# Klasyfikacja na bazie modelu regresji liniowej z czynnikami wielomianowymi
Trudności związane z maskowaniem klas znacząco utrudniają stworzenie efektywnego klasyfikatora opartego na modelu regresji liniowej. W celu zminimalizowania tego problemu i poprawy jakości klasyfikacji, wykorzystamy predyktory do wygenerowania czynników wielomianowych, czyli wyrażeń w formie: ...
$$
\begin{aligned}
& X_{1}^{t_{1}} X_{2}^{t_{2}} \dots X_{p}^{t_{p}}, \\
& \text{gdzie } \sum_{i=1}^{p} t_{i} = 2 \quad \text{oraz} \quad \forall i \in \{1, \dots, p\} \quad t_{i} \geq 0
\end{aligned}
$$
```{r, polynomial features creating}

# Creating the polynomial features
iris_df$Sepal.Length2 <- iris_df$Sepal.Length^2
iris_df$Sepal.Width2 <- iris_df$Sepal.Width^2
iris_df$Petal.Length2 <- iris_df$Petal.Length^2
iris_df$Petal.Width2 <- iris_df$Petal.Width^2

iris_df$Sepal.Length_Sepal.Width <- iris_df$Sepal.Length * iris_df$Sepal.Width
iris_df$Sepal.Length_Petal.Length <- iris_df$Sepal.Length * iris_df$Petal.Length
iris_df$Sepal.Length_Petal.Width <- iris_df$Sepal.Length * iris_df$Petal.Width
iris_df$Sepal.Width_Petal.Length <- iris_df$Sepal.Width * iris_df$Petal.Length
iris_df$Sepal.Width_Petal.Width <- iris_df$Sepal.Width * iris_df$Petal.Width
iris_df$Petal.Length_Petal.Width <- iris_df$Petal.Length * iris_df$Petal.Width

```

```{r, poly iris df splitting}
train_indices <- sample(seq_len(nrow(iris_df)), size = split_ratio * nrow(iris_df))

iris_df <-   iris_df %>% mutate(Predicted.Species = NULL)

train_data <- iris_df[train_indices, ]
test_data <- iris_df[-train_indices, ]

```

```{r, poly linear regression building}
# Train a model for each class

# Function to fit one-vs-rest linear regression model for a given class


model_fitter <- function(train_data, cls) {
  binary_response <- ifelse(train_data$Species == cls, 1, 0)
  lm(binary_response ~ ., data = train_data[, names(train_data) != "Species"])
}

models <- lapply(classes, function(cls) model_fitter(train_data, cls))


```

```{r iris df poly linear regression predicting}
# Predict on data
predicted.classes <- predict_multiclass(iris_df, models)

iris_df$Predicted.Species <- predicted.classes

```


```{r confussion matrix for training samples with polynomial features, fig.cap = "Macierz pomyłek regresji liniowej z cechami wielomianowymi", include = TRUE}
# Find the true labels
G <- iris_df[train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek treningowych",
       subtitle = "Z czynnikami wielomianowymi",
       x = "Predicted Class", y = "True Class", fill = "Precision") +
  theme_minimal()

```



```{r confussion matrix for testing samples with polynonomial features, fig.cap = "Macierz pomyłek regresji liniowej z cechami wielomianowymi", include = TRUE}
# Find the true labels
G <- iris_df[-train_indices, "Species"]

# Find the predicted labels
G.hat <- predicted.classes[-train_indices]

# Create a confussion matrix
conf.matrix <- confusionMatrix(data = G.hat, reference = G)$table

# Normalize the matrix by columns and melt the matrix
conf.matrix <- melt(prop.table(conf.matrix, margin = 2))

# Rename the columns of the matrix
colnames(conf.matrix) <- c("Predicted.label", "True.label", "Precision")


# Plot the confussion matrix as heatmap
ggplot(conf.matrix, aes(x = Predicted.label, y = True.label, fill = Precision)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(Precision, 2)), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Macierz pomyłek dla regresji liniowej dla próbek testowych",
       subtitle  = "Z czynnikami wielomianowymi",
       x = "Prognozowana klasa", y = "Rzeczywista klasa", fill = "Precision") +
  theme_minimal()

```
## Wnioski
Analiza map cieplnych macierzy pomyłek wykazała znaczącą poprawę skuteczności klasyfikacji po zastosowaniu modelu regresji liniowej z uwzględnieniem cech wielomianowych. W odróżnieniu od modelu bazowego, w którym zaobserwowano problem maskowania klasy oraz niską skuteczność klasyfikacji próbek należących do klasy 'versicolor', rozszerzony model z cechami wielomianowymi charakteryzuje się niemal całkowitym wyeliminowaniem tych niekorzystnych zjawisk.

Wykresy macierzy pomyłek jednoznacznie wskazują, że wprowadzenie czynników wielomianowych przyczyniło się do lepszego rozdzielenia przestrzeni cech, co w konsekwencji umożliwiło modelowi regresji liniowej dokładniejsze przypisanie próbek do właściwych klas. Zanik "pomijania" klasy 'versicolor' oraz ogólnie wyższa koncentracja wartości na głównej diagonali macierzy pomyłek dla modelu z cechami wielomianowymi stanowią silne argumenty przemawiające za istotnością rozszerzenia zestawu cech o komponenty wielomianowe.

Na podstawie przeprowadzonych obserwacji można zatem wnioskować, że dodanie czynników wielomianowych do modelu regresji liniowej jest uzasadnione i korzystnie wpływa na zdolności klasyfikacyjne modelu w analizowanym problemie. Rozszerzenie przestrzeni cech o interakcje i potęgi oryginalnych cech dostarcza modelowi dodatkowych informacji, które pozwalają na tworzenie bardziej złożonych i dokładnych granic decyzyjnych między klasami.

# Porównanie metod klasyfikacji

```{r, pimaindiansdiabetes loading}
library("mlbench")
data("PimaIndiansDiabetes2")
pimaindians.df <- PimaIndiansDiabetes2

```
W tym rozdziale skupimy się na porównaniu wybranych metod klasyfikacyjnych: algorytmu K-najbliższych sąsiadów, drzewa decyzyjnego oraz klasyfikatora bayesowskiego. Reguły decyzyjne zostaną wytrenowane i przetestowane na zbiorze danych PimaIndiansDiabetes2, który jest dostępny w pakiecie mlbench w języku R. Przed przystąpieniem do budowy modeli podzielimy dane na zbiór treningowy i testowy, aby zapobiec zjawisku wycieku danych oraz zapewnić wiarygodną ocenę skuteczności klasyfikatorów. 

Zbiór danych PimaIndiansDiabetes2 zawiera `r nrow(pimaindians.df)` przypadków oraz `r ncol(pimaindians.df)` zmiennych, z czego `r ncol(pimaindians.df)-1` stanowi potencjalne cechy predykcyjne, a jedna kolumna – diabetes – pełni rolę zmiennej objaśnianej. Określa ona, czy dana osoba – kobieta pochodząca z rdzennego plemienia Pima, zamieszkującego stan Arizona w USA – należy do grupy osób chorujących na cukrzycę typu 2. W celu uproszczenia analizy, poziomy zmiennej diabetes zostały zmienione z "pos" i "neg" na "1" i "0", przy czym zmienna zachowała typ czynnika (ang. factor).



```{r target var transforming}
pimaindians.df  <-  pimaindians.df %>% mutate(diabetes = ifelse(pimaindians.df$diabetes == "pos", 1, 0))

pimaindians.df$diabetes <- factor(pimaindians.df$diabetes)
```
Wstępna analiza opisowa zbioru danych wykazała, że brakujące wartości zostały oznaczone w sposób zgodny z powszechną konwencją, czyli przy użyciu symbolu NA. W danych nie występują nietypowe lub niepoprawne sposoby kodowania braków, takie jak wartość 0 w kolumnie insulin, co czasem spotyka się w gorzej przygotowanych zbiorach. Zmienna docelowa diabetes jest prawidłowo przechowywana jako czynnik (ang. factor), co umożliwia bezpośrednie jej wykorzystanie w zadaniach klasyfikacyjnych.

## Wstępna analiza danych.
Na początku przeprowadzona zostanie analiza rozkładu klas zmiennej docelowej. Jest to ważny krok, gdyż nierównomierne rozłożenie klas (tzw. problem niezbalansowanych danych) może znacząco wpłynąć na ocenę skuteczności modeli.
```{r, target classes distribution, fig.cap = "Rozkład etykiet zmiennej celu diabetes", include = TRUE}
# Probability for each class of `diabetes` target variable.
probs <- pimaindians.df %>%
  group_by(diabetes) %>%
  summarise(liczba = n()) %>%
  mutate(prawdopodobienstwo = liczba / sum(liczba))

probs$diabetes <- factor(probs$diabetes)

ggplot(probs, aes(x = diabetes, y = prawdopodobienstwo, fill = diabetes)) +
  geom_bar(stat = "identity") +
  labs(x = "Klasa", y = "Odsetek", title = "Rozkład etykiet zmiennej `Diabetes`")

```
Z rysunku \ref{fig:target classes distribution} wyraźnie wynika, że mamy do czynienia z problemem niezbalansowanych danych — liczba osób niechorujących na cukrzycę typu 2 jest niemal dwukrotnie większa niż liczba osób chorych. Teoretycznie, gdybyśmy zastosowali prosty, „naiwny” klasyfikator przypisujący każdą obserwację do klasy dominującej, uzyskalibyśmy wysoką ogólną skuteczność. Jednak po bliższej analizie metryk oceniających dokładność dla obu klas okazałoby się, że taki model w praktyce jest nieskuteczny, ponieważ całkowity błąd klasyfikacji dla klasy mniejszościowej wyniósłby 100%.

Przeanalizujemy teraz rozkłady ciągłych predyktorów. Jest to bardzo istotna kwestia, zwłaszcza w kontekście klasyfikatora KNN, gdzie różne skale pomiarowe różnych zmiennych mogą znacząco zanizać wpływ zmiennych charakteryzujących się zwięzłym zakresem wartości.

```{r variance comparison, fig.cap = "Porównanie wariancji predyktorów", include = TRUE}
# Find predictors
predictors <- setdiff(colnames(pimaindians.df), "diabetes")

# Dataframe with predictors
X <- pimaindians.df[predictors]

# Convert to long format
X.long <- X %>%
  pivot_longer(cols = everything(),
               names_to = "predictor",
               values_to = "realization")

# Plotting boxplots
ggplot(X.long, aes(x = predictor, y = realization, fill = predictor)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # rotate x-axis labels 45 degrees for better readability
    legend.position = "none"                             # hide legend (fill color is redundant with x-axis labels)
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów"               # tytuł wykresu
  )

```
Z rysunku \ref{fig:variance comparison} jasno wynika, że standaryzacja predyktorów jest niezbędna. Zmienne różnią się istotnie zarówno pod względem wariancji, jak i wartości tendencji centralnej, co szczególnie widoczne jest na przykładzie porównania wykresów pudełkowych zmiennych „insulin” oraz „triceps”. Dokonajmy zatem standaryzacji i spójrzmy na rezultaty, które przedstawiono na rysunku \ref{fig:variance comparison after standarization}

```{r variance comparison after standarization, fig.cap = "Porównanie wariancji predyktorów po zastosowaniu standaryzacji", include = TRUE}

X.std <- data.frame(apply(X, 2, function(x){ (x - mean(x, na.rm = TRUE))/ sd(x, na.rm = TRUE)}
))

# Convert to long format
X.std.long <- X.std %>%
  pivot_longer(cols = everything(),
               names_to = "predictor",
               values_to = "realization")


# Plotting boxplots
ggplot(X.std.long, aes(x = predictor, y = realization, fill = predictor)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # rotate x-axis labels 45 degrees for better readability
    legend.position = "none"                             # hide legend (fill color is redundant with x-axis labels)
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów",               # tytuł wykresu
    subtitle = "Po zastosowaniu standaryzacji"            # Podtytuł wykresu
  )


```
Mając już ujednoliconą skalę pomiarową dla wszystkich predyktorów, możemy przejść do oceny ich zdolności dyskryminacyjnej. Ten etap pozwoli nam zidentyfikować zmienne najlepiej rozróżniające klasy i lepiej zrozumieć ich wpływ na działanie modelu.
```{r dyscrimination power analysis, include = TRUE, fig.width= 7, fig.height = 6, fig.cap = "Porównanie zdolności dyskryminacyjnych predyktorów"}
# Apply the standarization to the dataframe
pimaindians.df[predictors] = X.std


pimaindians.long.df <- pimaindians.df %>% pivot_longer( cols = where(is.numeric), values_to = "realization", 
                                                        names_to = "predictor")

# Plotting boxplots
ggplot(pimaindians.long.df, aes(x = predictor, y = realization, fill = diabetes)) +
  geom_boxplot(outlier.size = 1, outlier.alpha = 0.6) +  # boxplot with smaller, semi-transparent outliers
  theme_minimal(base_size = 14) +                        # clean minimal theme with base font size 14
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)  # rotate x-axis labels 45 degrees for better readability
  ) +
  labs(
    x = "Predyktor",                                     # etykieta osi X
    y = "Realizacja zmiennej",                           # etykieta osi Y
    title = "Wykres pudełkowy predyktorów po zastosowaniu standaryzacji",               # tytuł wykresu
  )


```
Jak pokazano na rysunku \ref{fig:dyscrimination power analysis}, żadna ze zmiennych nie rozdziela klas docelowych w sposób idealny. Mimo to, można wyróżnić predyktory o wyraźnie silniejszych zdolnościach dyskryminacyjnych. Do takich atrybutów należą zmienne: 'glucose', 'insulin', 'triceps' oraz 'mass'. Natomiast najsłabszą zdolność separacji klas wykazują zmienne 'pedigree' oraz 'pressure'.

# Ocena dokładności klasyfikacji i porównanie metod {#wykresy}

Celem niniejszej części analizy jest szczegółowe porównanie trzech wybranych algorytmów klasyfikacyjnych: K-najbliższych sąsiadów (KNN), naiwnego klasyfikatora Bayesowskiego oraz drzewa decyzyjnego. Aby zapewnić obiektywną i rzetelną ocenę skuteczności poszczególnych metod, wszystkie modele zostaną wytrenowane oraz przetestowane na tym samym podziale danych. Dzięki temu możliwe będzie bezpośrednie porównanie ich wyników, przy zachowaniu jednolitych warunków eksperymentu.

 
```{r dataset splitting}
split_ratio <- 0.7
set.seed(123)

# Imputation by mean
pimaindians.df <- pimaindians.df %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))


train_indices <- sample(seq_len(nrow(pimaindians.df)), size = split_ratio * nrow(pimaindians.df))

train_data <- pimaindians.df[train_indices, ]
test_data  <- pimaindians.df[-train_indices, ]


train.X <- train_data[, predictors] # Training set of predictors
train.Y <- train_data[, c("diabetes")] # Training set of target variable

test.X <- test_data[, predictors] # Testing set of predictors
test.Y <- test_data[, c("diabetes")]  # Testing set of target variable

```

```{r KNN training, include = TRUE}
# Build the KNN model
KNN_build <- function(train.X, test.X, train.Y, k){
  knn_pred <- knn(train = train.X, test = test.X, cl = train.Y, k = k)
  return (knn_pred)
}

knn_pred <- KNN_build(train.X, test.X, train.Y, 5)
```

```{r classification tree, include=TRUE}
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class")
tree_pred <- predict(tree_model, test.X, type = "class")

```

```{r naive bayes classifier, include=TRUE}
nb_model <- naiveBayes(x = train.X, y = train.Y)
nb_pred <- predict(nb_model, newdata = test.X)
```

```{r confusion matrixes for algorithms, include=TRUE}

plot_confusion_matrix <- function(predictions, true_labels, model_name = "Model", subtitle = "") {
  
  predictions <- as.factor(predictions)
  true_labels <- as.factor(true_labels)
  levels(predictions) <- levels(true_labels)
  
  conf_raw <- confusionMatrix(data = predictions, reference = true_labels)
  conf_table <- conf_raw$table
  
  conf_melted <- melt(prop.table(conf_table, margin = 2))
  colnames(conf_melted) <- c("Predicted.label", "True.label", "Precision")
  
  if (all(levels(true_labels) %in% c("0", "1"))) {
    conf_melted$Predicted.label <- factor(conf_melted$Predicted.label, levels = c("0", "1"))
    conf_melted$True.label <- factor(conf_melted$True.label, levels = c("0", "1"))
  }

  p <- ggplot(conf_melted, aes(x = Predicted.label, y = True.label, fill = Precision)) +
    geom_tile(color = "white") +
    geom_text(aes(label = round(Precision, 2)), size = 4) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = paste("Macierz pomyłek -", model_name),
         subtitle = subtitle,
         x = "Prognozowana klasa", y = "Rzeczywista klasa", fill = "Precision") +
    theme_minimal()
  
  class_precisions <- filter(conf_melted, Predicted.label == True.label)
  precision.0 <- class_precisions[class_precisions$True.label == "0", "Precision"]
  precision.1 <- class_precisions[class_precisions$True.label == "1", "Precision"]

  return(list(
    plot = p,
    precision.0 = precision.0,
    precision.1 = precision.1
  ))
}


knn_result <- plot_confusion_matrix(knn_pred, test.Y, model_name = "KNN", subtitle = "k = 5")
tree_result <- plot_confusion_matrix(tree_pred, test.Y, model_name = "Drzewo decyzyjne")
nb_result   <- plot_confusion_matrix(nb_pred, test.Y, model_name = "Naiwny Bayes")

```

```{r KNN evaluating, fig.cap = "Macierz pomyłek dla algorytmu KNN, k = 5", include = TRUE}
print(knn_result$plot)
```

```{r clasification tree evaluating, fig.cap = "Macierz pomyłek dla drzewa klasyfikacyjnego", include = TRUE}
print(tree_result$plot)
```

```{r naive bayes evaluating, fig.cap = "Macierz pomyłek dla naiwnego klasyfikatora bayesa", include = TRUE}
print(nb_result$plot)
```

Analizując [macierze pomyłek](#wykresy) dla wybranych algorytmów można zauważyć, że najlepsze wyniki uzyskał algorytm KNN przy liczbie sąsiadów 'k=5'. Wartość precyzji tego modelu dla klasy 0 wynosi `r round(knn_result$precision.0,2)` natomiast dla klasy 1 - `r round(knn_result$precision.1,2)`. Wskazuje to na jego wysoką skuteczność w rozpoznawaniu zarówno dominującej, jak i mniejszościowej klasy. W porównaniu do pozostałych metod KNN popełnił najmniej błędów klasyfikacyjnych, co czyni go najbardziej trafnym algorytmem dla analizowanego przypadku.

Drzewo decyzyjne osiągnęło nieco gorsze wyniki. Precyzja w tym przypadku wynosi `r round(tree_result$precision.0,2)` dla klasy 0 oraz `r round(tree_result$precision.1,2)` dla klasy 1. Choć jego skuteczność była niższa niż w przypadku KNN, to nadal pozostaje ona na akceptowalnym poziomie, szczególnie biorąc pod uwagę prostotę interpretacji struktury drzewa decyzyjnego.

Najniższą skuteczność wykazał naiwny klasyfikator Bayesa, który poprawnie rozpoznał `r round(nb_result$precision.0,2)` przypadków klasy 0 oraz `r round(nb_result$precision.1,2)` przypadków klasy 1. Chociaż jego wyniki są zbliżone do drzewa decyzyjnego, nie oferuje on wyraźnych przewag nad pozostałymi modelami.

Warto jednak zwrócić uwagę, że zaprezentowane wyniki odnoszą się do konkretnej konfiguracji parametrów modeli. Można zatem sprawdzić, czy możliwe jest uzyskanie większej skuteczności po odpowiednim dobraniu parametrów w powyższych algorytmach. W tym celu porównamy błędy klasyfikacji algorytmu dla różnych wartości parametrów, najpierw stosując jednokrotny podział danych na zbiór treningowy i testowy, a następnie wykorzystując walidację krzyżową. Podejście to pozwoli na bardziej obiektywną ocenę efektywności modeli.


```{r classification error, include=TRUE}
plot_classification_error_with_train <- function(param_values, model_function, param_name = "Parametr", model_label = "Model") {

  errors <- lapply(param_values, function(p) {
    preds <- model_function(p)
    train_error <- mean(preds$train != train.Y)
    test_error <- mean(preds$test != test.Y)
    
    return(data.frame(param = p, TrainError = train_error, TestError = test_error))
  })
  
  df <- do.call(rbind, errors)
  df_long <- reshape2::melt(df, id.vars = "param", variable.name = "Dataset", value.name = "Error")
 
  min_test_error <- min(df$TestError)
  best_param <- df$param[which.min(df$TestError)]
  
  highlight_point <- data.frame(param = best_param, Error = min_test_error, Dataset = "TestError")
  
  p <- ggplot(df_long, aes(x = param, y = Error, color = Dataset)) +
    geom_line(size = 1) +
    geom_point(size = 2) +
    geom_point(data = highlight_point, aes(x = param, y = Error), 
               color = "green", size = 4, shape = 18) +  # green point for the smallest error
    labs(
      title = paste("Błąd klasyfikacji w zależności od", param_name),
      subtitle = paste("Model:", model_label),
      x = param_name,
      y = "Błąd klasyfikacji"
    ) +
    scale_color_manual(values = c("TrainError" = "blue", "TestError" = "red"),
                       labels = c("Zbiór treningowy", "Zbiór testowy")) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11),
      axis.title = element_text(size = 12),
      axis.text = element_text(angle = 45, size = 10),
      legend.title = element_blank()
    )
  

  return(list(
    plot = p,
    data = df,
    min_test_error = min_test_error,
    best_param = best_param
  ))
}



```

```{r Classification error of KNN, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji KNN dla różnych wartości hiperparametru k"}
model_function_knn <- function(k) {
  train_pred <- knn(train.X, train.X, cl = train.Y, k = k)
  test_pred <- knn(train.X, test.X, cl = train.Y, k = k)
  
  return(list(train = train_pred, test = test_pred))
}

result <- plot_classification_error_with_train(1:20, model_function_knn, param_name = "Liczba sąsiadów", model_label = "KNN")
print(result$plot)

```


```{r Classification error of the tree, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji drzewa decyzyjnego dla różnych wartości parametru cp"}

tree_model_function_cp <- function(cp_value) {
  model <- rpart(train.Y ~ ., data = train.X, method = "class",
                 control = rpart.control(cp = cp_value))
  
  train_pred <- predict(model, train.X, type = "class")
  test_pred <- predict(model, test.X, type = "class")
  
  return(list(train = train_pred, test = test_pred))
}

result_cp <- plot_classification_error_with_train(
  param_values = seq(0.0001, 0.05, by = 0.002),  # przykładowy zakres cp
  model_function = tree_model_function_cp,
  param_name = "Parametr cp",
  model_label = "Drzewo decyzyjne"
)

# Wyświetlenie wykresu
print(result_cp$plot)

```
```{r Classification naive bayes , include = TRUE, fig.cap = "Porównanie błędu klasyfikacji naiwnego klasyfikatora bayesowskiego dla różnych wartości parametru laplace"}
library(naivebayes)
model_nb_adjust <- function(adjust_val) {
  model <- naive_bayes(x = train.X, y = train.Y, usekernel = TRUE, adjust = adjust_val)
  
  pred_train <- predict(model, train.X)
  pred_test <- predict(model, test.X)
  
  return(list(train = pred_train, test = pred_test))
}

param_values <- seq(0.1, 2, by = 0.3)

result_nb <- plot_classification_error_with_train(param_values, model_nb_adjust,
                                              param_name = "Adjust",
                                              model_label = "Naive Bayes (kernel smoothing)")

print(result_nb$plot)

```


```{r cross validation, include = TRUE}
cv_model_error_plot <- function(train.X, train.Y, method, param_name, param_values,
                                folds = 25, seed = 123, model_label = NULL) {
  
  train_data <- data.frame(X = train.X, Y = as.factor(train.Y))
  ctrl <- trainControl(method = "cv", number = folds)
  
  tune_grid <- expand.grid(param_values)

  
  if (length(param_name) == 1 && ncol(tune_grid) == 1) {
    colnames(tune_grid) <- param_name
  }

  set.seed(seed)
  model_cv <- train(Y ~ ., data = train_data,
                    method = method,
                    trControl = ctrl,
                    tuneGrid = tune_grid)

  results <- model_cv$results
  results$ClassificationError <- 1 - results$Accuracy

  # Obsługa wyboru param_name do wykresu
  best_row <- which.min(results$ClassificationError)
  best_param_val <- results[[param_name]][best_row]
  min_CE <- results$ClassificationError[best_row]
  
  if (is.null(model_label)) model_label <- method
  
  p <- ggplot(results, aes_string(x = param_name, y = "ClassificationError")) +
    geom_line(color = "blue") +
    geom_point(color = "red", size = 2.5) + 
    geom_point(data = subset(results, results[[param_name]] == best_param_val),
               aes_string(x = param_name, y = "ClassificationError"),
               color = "green", size = 2.5) +
    labs(title = paste("Błąd klasyfikacji w cross-validation dla", model_label),
         x = param_name,
         y = "Błąd klasyfikacji") +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_minimal()
  
  return(list(
    best_param = best_param_val,
    min_error = min_CE,
    plot = p
  ))
}

```

```{r cross validation for KNN, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji KNN dla różnych wartości hiperparametru k z uwzględnieniem walidacji krzyżowej"}
result_knn <- cv_model_error_plot(train.X, train.Y,
                                 method = "knn",
                                 param_name = "k",
                                 param_values = 1:25,
                                 model_label = "KNN")

print(result_knn$plot)

```

```{r cross validation for classification tree, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji drzewa  klasyfikacyjnego dla różnych wartości parametru cp z uwzględnieniem walidacji krzyżowej"}
result_tree <- cv_model_error_plot(train.X, train.Y,
                                  method = "rpart",
                                  param_name = "cp",
                                  param_values = seq(0.001, 0.05, by = 0.005),
                                  model_label = "Drzewo decyzyjne")

print(result_tree$plot)


```

```{r cross validation for naive bayes, include = TRUE, fig.cap = "Porównanie błędu klasyfikacji naiwnego klasyfikatora bayesa dla różnych wartości parametru laplace z uwzględnieniem walidacji krzyżowej"}

param_grid <- expand.grid(
  laplace = 1,
  usekernel = TRUE,
  adjust = seq(0.1, 2, by = 0.3)
)

result_cv_nb <- cv_model_error_plot(
  train.X = train.X,
  train.Y = train.Y,
  method = "naive_bayes",
  param_name = "adjust",
  param_values = param_grid,
  model_label = "Naive Bayes (adjust)"
)

print(result_cv_nb$plot)
```

\newpage
Na wykresie \ref{fig:Classification error of KNN} przedstawiono zależność błędu klasyfikacji od wartości hiperparametru k w algorytmie KNN. Najmniejszy błąd klasyfikacji dla zbioru testowego uzyskano dla k = `r result$best_param`, a jego wartość wynosiła `r round(result$min_test_error,2)`.

Warto jednak zauważyć, że pojedynczy podział danych na zbiór treningowy i testowy może prowadzić do niestabilnych wyników – jest wrażliwy na sposób losowego podziału. Właśnie dlatego stosuje się walidację krzyżową, która uśrednia wyniki z wielu podziałów, co prowadzi do bardziej wiarygodnej oceny skuteczności modelu.

W przypadku walidacji krzyżowej, najmniejszy błąd klasyfikacji osiągnięto dla k = `r result_knn$best_param`, a jego wartość wyniosła `r round(result_knn$min_error,2)`. Choć wynik z walidacji krzyżowej może być nieco gorszy w konkretnym przypadku, jego średnia efektywność w różnych scenariuszach jest wyższa, co czyni go bardziej reprezentatywnym.

Na wykresie \ref{fig:Classification error of the tree} zaprezentowano wpływ wartości parametru cp (complexity parameter) na błąd klasyfikacji w drzewie decyzyjnym. Parametr ten określa minimalną wartość poprawy jakości podziału, wymaganą do wykonania kolejnego rozgałęzienia drzewa. Najniższy błąd klasyfikacji na zbiorze testowym zaobserwowano dla cp = `r round(result_cp$best_param,2)`, przy wartości błędu równej `r round(result_cp$min_test_error,2)`. Z kolei wyniki walidacji krzyżowej wskazują, że optymalna średnia wartość parametru to `r round(result_tree$best_param,2)`, dla której błąd klasyfikacji wyniósł `r round(result_tree$min_error,2)`.

Dla naiwnego klasyfikatora Bayesa analizowano wpływ parametru adjust, regulującego stopień wygładzania przy estymacji rozkładów jądrowych. Najlepszy wynik na zbiorze testowym osiągnięto przy adjust = `r result_nb$best_param`, a odpowiadający mu błąd klasyfikacji wyniósł `r round(result_nb$min_test_error,2)`. Z kolei walidacja krzyżowa wykazała, że najniższy błąd klasyfikacji (`r round(result_cv_nb$min_error,2)`) uzyskano przy adjust = `r result_cv_nb$best_param`.

## Nowy podzbiór zmiennych

Na podstawie wstępnej analizy zdolności dyskryminacyjnych zmiennych ze zbioru danych PimaIndiansDiabetes2, można wyodrębnić podzbiór cech najlepiej różnicujących obserwacje - w naszym przypadku będą to zmienne: "glucose", "mass" oraz "insulin". Taki podzbiór następnie posłuży do kontynuacji porównania skuteczności trzech wybranych algorytmów klasyfikacyjnych: k-najbliższych sąsiadów (k-NN), drzewa decyzyjnego oraz naiwnego klasyfikatora Bayesa.


```{r new data set}
split_ratio <- 0.7
set.seed(123)

pimaindians.df2 <- pimaindians.df[, c("glucose", "mass", "insulin", "diabetes")]

train_indices <- sample(seq_len(nrow(pimaindians.df2)), size = split_ratio * nrow(pimaindians.df2))

train_data <- pimaindians.df2[train_indices, ]
test_data  <- pimaindians.df2[-train_indices, ]


train.X <- train_data[, c("glucose", "mass", "insulin"), drop = FALSE]
train.Y <- train_data[, c("diabetes")] 

test.X <- test_data[, c("glucose", "mass", "insulin"), drop = FALSE] 
test.Y <- test_data[, c("diabetes")]  
```

```{r KNN evaluating2, fig.cap = "Macierz pomyłek dla algorytmu KNN, k = 5, dla nowego podzbioru danych", include = TRUE}

knn_pred <- KNN_build(train.X, test.X, train.Y, 5)

knn_result <- plot_confusion_matrix(knn_pred, test.Y, model_name = "KNN", subtitle = "k = 5")
print(knn_result$plot)


```
```{r tree evaluating2, fig.cap = "Macierz pomyłek dla drzewa klasyfikacyjnego dla nowego podzbioru danych", include = TRUE}

tree_model <- rpart(diabetes ~ ., data = train_data, method = "class")
tree_pred <- predict(tree_model, test.X, type = "class")
tree_result <- plot_confusion_matrix(tree_pred, test.Y, model_name = "Drzewo decyzyjne")
print(tree_result$plot)
```
```{r nb evaluating2, fig.cap = "Macierz pomyłek dla naiwnego klasyfikatora bayesa dla nowego podzbioru danych", include = TRUE}

nb_model <- naiveBayes(x = train.X, y = train.Y)
nb_pred <- predict(nb_model, newdata = test.X)
nb_result   <- plot_confusion_matrix(nb_pred, test.Y, model_name = "Naiwny Bayes")
print(nb_result$plot)

```

Po wybraniu nowego podzbioru, mimo że składa się on ze zmiennych o najlepszych własnościach dyskryminacyjnych, może dojść do pogorszenia jakości klasyfikacji w porównaniu do modelu trenowanego na pełnym zestawie cech.

Redukcja liczby zmiennych oznacza utratę części informacji, która — choć może wydawać się mniej istotna pojedynczo — w połączeniu z innymi cechami pomaga modelowi lepiej rozróżniać klasy. Czasem dodatkowe zmienne zawierają wzorce lub korelacje, które poprawiają zdolność predykcyjną modelu.
Dodatkowo zmniejszenie wymiarowości danych może wpływać na stabilność i uogólnialność modelu, zwłaszcza w przypadku metod takich jak k-najbliższych sąsiadów czy drzew decyzyjnych, które korzystają z pełnego obrazu przestrzeni cech.

Warto jednak zauważyć, że mimo tych ograniczeń, modele: drzewa decyzyjnego oraz naiwny klasyfikator bayesa wyraźnie lepiej radzą sobie z klasyfikacją klasy 0 na ograniczonym podzbiorze cech. Szczególnie drzewo klasyfikacyjne osiąga wysoką dokładność — 'około 91%' — co sugeruje, że wybrane cechy istotnie wspierają rozpoznawanie tej klasy. Taki wynik wskazuje, że selekcja cech, choć powoduje pewną utratę ogólnej efektywności, może poprawić wydajność w kontekście konkretnych zadań lub klas.

# Końcowe wnioski - podsumowanie

Najlepsze wyniki uzyskano korzystając z pełnego zestawu zmiennych predykcyjnych, co pozwoliło modelom na wykorzystanie wszystkich dostępnych informacji. W przypadku algorytmu k-najbliższych sąsiadów (KNN) optymalna liczba sąsiadów wyniosła `r result_knn$best_param` (walidacja krzyżowa), co przełożyło się na najniższy błąd klasyfikacji `r round(result_knn$min_error, 2)`. Dla drzewa decyzyjnego najlepszą wartością parametru złożoności (cp) była `r round(result_tree$best_param, 2)`, a dla naiwnego klasyfikatora Bayesa optymalny stopień wygładzania (adjust) wyniósł `r result_cv_nb$best_param`.

Selekcja podzbioru cech ograniczona do "glucose", "mass" oraz "insulin" poprawiła skuteczność klasyfikacji klasy 0, zwłaszcza w drzewie decyzyjnym, gdzie dokładność klasyfikacji tej klasy osiągnęła około 91%. Jednakże w przypadku pełnego zestawu cech modele osiągały lepsze ogólne wyniki, co wskazuje na wartość zachowania wszystkich dostępnych informacji.

Najlepsze rezultaty w analizie uzyskał algorytm KNN, który charakteryzuje się największą precyzją oraz najniższym błędem klasyfikacji zarówno dla klasy dominującej (0), jak i mniejszościowej (1). Drzewo decyzyjne zapewnia wyniki na poziomie akceptowalnym, szczególnie dobrze radząc sobie z klasyfikacją klasy 0, co jest istotne w kontekście interpretowalności modelu. Naiwny klasyfikator Bayesa wykazał najniższą skuteczność, choć jego wyniki nadal pozostają zbliżone do wyników drzewa, co czyni go potencjalnie użytecznym w niektórych zastosowaniach.

Wybór schematu oceny miał znaczący wpływ na wnioski dotyczące skuteczności poszczególnych metod. Wyniki uzyskane na podstawie jednokrotnego podziału danych były bardziej niestabilne i wrażliwe na losowość podziału, co mogło prowadzić do błędnych lub mylących wniosków. Wykorzystanie walidacji krzyżowej pozwoliło uśrednić wyniki z wielu podziałów, dzięki czemu ocena skuteczności modeli stała się bardziej rzetelna i reprezentatywna. W praktyce oznacza to, że parametry wybrane na podstawie walidacji krzyżowej lepiej generalizują na nowe, nieznane dane, a decyzje o wyborze najlepszych modeli są bardziej wiarygodne.

